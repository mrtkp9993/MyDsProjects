[
  {
    "objectID": "EarthQuakeProbability/Analysis.html",
    "href": "EarthQuakeProbability/Analysis.html",
    "title": "Modelling the probability of earthquakes (M >= 5.0) in North Anatolian Fault Zone",
    "section": "",
    "text": "—\nThanks for reading. If you enjoyed, please share it.\nSupport my work from here GitHub Sponsors."
  },
  {
    "objectID": "EarthQuakeProbability/Analysis.html#eda",
    "href": "EarthQuakeProbability/Analysis.html#eda",
    "title": "Modelling the probability of earthquakes (M >= 5.0) in North Anatolian Fault Zone",
    "section": "EDA",
    "text": "EDA\nLet’s look and visualize the historical earthquake data.\n\n\n\n\n  \n\n\n\n\n\nTime Span between Earthquake Occurrences\n\n```{r}\np <- ggplot(data_diff_between_eq, aes(x=diff)) + geom_histogram(aes(y = ..density..)) + geom_density()\np\n```\n\n\n\n\n\n```{r}\np2 <- ggplot(data_diff_between_eq, aes(x=diff)) + geom_boxplot()\np2\n```\n\n\n\n\n\n\nEarthquake Count By Year\n\n```{r}\np3 <- ggplot(data_count_by_year, aes(x=year, y=count)) + geom_line()\np3\n```"
  },
  {
    "objectID": "EarthQuakeProbability/Analysis.html#modelling-the-probability",
    "href": "EarthQuakeProbability/Analysis.html#modelling-the-probability",
    "title": "Modelling the probability of earthquakes (M >= 5.0) in North Anatolian Fault Zone",
    "section": "Modelling the probability",
    "text": "Modelling the probability\nLet’s fit Weibull distribution to distribution of days between two earthquakes occurred successively.\n\n```{r}\nplotdist(data_diff_between_eq$diff, demp = TRUE)\n```\n\n\n\n\nFit the distribution:\n\n```{r}\n# add all data points to 0.1 for avoiding zero division errors\ndata_diff_between_eq$diff <- data_diff_between_eq$diff + 0.01\n\nwei.fit <- fitdist(data_diff_between_eq$diff, \"weibull\")\n```\n\nCheck convergence, 0 means procedure was converged:\n\n```{r}\nprint(wei.fit$convergence)\n```\n\n[1] 0\n\n\nResults:\n\nEstimate\n\n\n\n\nx\nsd\n\n\n\n\nshape\n0.3647605\n0.0294541\n\n\nscale\n124.4717945\n34.3619153\n\n\n\nFit quality:\n\n\n\n\nvalue\n\n\n\n\nloglik\n-618.5651\n\n\naic\n1241.13\n\n\nbic\n1246.494\n\n\n\nPlots:\n\n\n\n\n\n\nLet’s calculate mean occurence period of earthquakes which have magnitudes equal or bigger than 5 (simulation and theoretical mean):\n\n```{r}\nshape.v <- as.numeric(wei.fit$estimate[1])\nscale.v <- as.numeric(wei.fit$estimate[2])\n\nsimulated_data <- rweibull(100000, shape = shape.v, scale = scale.v)\n```\n\n\n\n\n\nvalue\n\n\n\n\nSimulated mean\n535.6679525\n\n\nTheoretical mean\n545.0535056\n\n\n\nIt is expected to have another earthquake having magnitude equal to 5 or above are average 545 days later than the preceding one.\nLet’s plot the CDF:\n\n```{r}\nplot(ecdf(simulated_data), xlim=c(0, 6000))\n```\n\n\n\n\nIf we look the data, the last earthquake was occurred at 2006-10-24, so 5753 days passed since last earthquake was occurred. The risk of an earthquake happening today is 98 %.\nFull source code: https://github.com/mrtkp9993/MyDsProjects/tree/main/EarthQuakeProbability"
  },
  {
    "objectID": "EarthQuakeProbability/Analysis.html#references",
    "href": "EarthQuakeProbability/Analysis.html#references",
    "title": "Modelling the probability of earthquakes (M >= 5.0) in North Anatolian Fault Zone",
    "section": "References",
    "text": "References\n\nEarthquake historical data was downloaded from: http://deprem.afad.gov.tr/depremkatalogu\nMap shape file was downloaded from: https://gadm.org/\nWeibull distribution in earthquake probability modelling: Yilmaz, Veysel & Erişoğlu, Murat & Çelik, H.. (2004). Probabilistic Prediction of the Next Earthquake in The Nafz (North Anatolian Fault Zone), Turkey = Kuzey Anadolu Fay Zonunda (Nafz) Gelecek Depremlerin Olasılıksal Tahmini. Dogus University Journal. 5.\nWeibull distribution fitting: https://stats.stackexchange.com/questions/230937/how-to-find-initial-values-for-weibull-mle-in-r"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Data Science Blog & Projects",
    "section": "",
    "text": "—\nThanks for reading. If you enjoyed, please share it.\nSupport my work from here GitHub Sponsors."
  },
  {
    "objectID": "MissingData/Analysis.html",
    "href": "MissingData/Analysis.html",
    "title": "Don’t impute all missing data: Missing Data Patterns",
    "section": "",
    "text": "A missing data pattern is the structure of observed and missing values in a data set. This is not to be confused with a missing data mechanism, which describes possible relationships between data and an one’s propensity for missing values. Patterns describe where the gaps in the data are, whereas mechanisms explain why the values are missing.\n\nThe missing values in panel a have been isolated on a single variable in the univariate pattern. This pattern could appear, for example, in an experimental setting where outcome scores for a subset of participants are missing. Panel b depicts a monotone missing data pattern from a longitudinal study in which individuals who have missing data at one measurement event always have missing data at subsequent measurements. The general pattern in panel c is that missing values are scattered all through the entire data matrix. Panel d depicts a planned missing data pattern in which three variables are intentionally left blank for a large number of respondents. Panel e depicts a pattern in which a latent variable is absent across the entire sample.\nOne final configuration needs special consideration because it may introduce estimation issues for modern missing data-handling procedures. Because the data provide insufficient support for estimation, I refer to the configuration in panel f as an underidentified missing pattern. This pattern frequently occurs when two categorical variables have unbalanced group sizes and missing data, resulting in very low or even zero cell counts in a cross-tabulation table. Prior to conducting a missing data analysis, it is critical to screen for this configuration.\n—\nThanks for reading. If you enjoyed, please share it.\nSupport my work from here GitHub Sponsors."
  },
  {
    "objectID": "MissingData/Analysis.html#missing-data-mechanisms",
    "href": "MissingData/Analysis.html#missing-data-mechanisms",
    "title": "Don’t impute all missing data: Missing Data Patterns",
    "section": "Missing Data Mechanisms",
    "text": "Missing Data Mechanisms\nAccording to Rubin, there are three different missing data mechanisms:\n\nMissing Completely at Random (MCAR): Values in a data set are missing completely at random (MCAR) if the events that lead to any particular data-item being missing are independent both of observable variables and of unobservable parameters of interest, and occur entirely at random. When data are MCAR, one can do listwise deletion and perfom analysis; the analysis performed on the data is unbiased; however, data are rarely MCAR.\nMissing at Random (MAR): Missing at random (MAR) occurs when the missingness is not random, but where missingness can be fully accounted for by variables where there is complete information. Since MAR is an assumption that is impossible to verify statistically, we must rely on its substantive reasonableness. Depending on the analysis method, these data can still induce parameter bias in analyses due to the contingent emptiness of cells. However, if the parameter is estimated with Full Information Maximum Likelihood, MAR will provide asymptotically unbiased estimates.\nMissing not at Random (MNAR): Missing not at random (MNAR) (also known as nonignorable nonresponse) is data that is neither MAR nor MCAR (i.e. the value of the variable that’s missing is related to the reason it’s missing). One can try to include as many predictors as possible in a model to get MNAR closer to MAR.\n\nAlso, one can perform multiple imputation (MI) methods in MCAR and MAR cases."
  },
  {
    "objectID": "MissingData/Analysis.html#how-to-test",
    "href": "MissingData/Analysis.html#how-to-test",
    "title": "Don’t impute all missing data: Missing Data Patterns",
    "section": "How to test?",
    "text": "How to test?\n\nDistinguish between MNAR and MAR\nThere is no statistical test for this, but you can:\n\nUse domain knowledge about variables;\nCollect more data for explaning missingness;\nDo literature search\n\nfor determining the missing data mechanisim.\n\n\nDistinguish between MCAR and MAR\nGenerally, two methods are preferred:\n\nLittle’s MCAR test: Maximum likelihood chi-square test for missing completely at random. \\(H_0\\) is that the data is MCAR.\nDummy variable approach for MCAR: One can create a dummy variable for whether a variable is missing (1 = missing, 0 = observed) and run t-tests (continuous) and chi-square (categorical) tests between this dummy and other variables to see if the missingness is related to the values of other variables."
  },
  {
    "objectID": "MissingData/Analysis.html#examples",
    "href": "MissingData/Analysis.html#examples",
    "title": "Don’t impute all missing data: Missing Data Patterns",
    "section": "Examples",
    "text": "Examples\nExample dataset from Fox, Negrete-Yankelevich and Sosa (2015):\n\n```{r}\nlibrary(readr)\nlibrary(naniar)\n\nex_data <- read_csv(\"ex_data.csv\", \n     col_types = cols(x_complete = col_factor(levels = c(\"1\", \n         \"2\", \"3\", \"4\")), x_mcar = col_factor(levels = c(\"1\", \n         \"2\", \"3\", \"4\")), x_mar = col_factor(levels = c(\"1\", \n         \"2\", \"3\", \"4\")), x_mnar = col_factor(levels = c(\"1\", \n         \"2\", \"3\", \"4\"))))\nex_data\n```\n\n\n\n  \n\n\n\nDescriptives:\n\n```{r}\nlibrary(finalfit)\n\nres <- ff_glimpse(ex_data, dependent = \"y\", explanatory = c(\"x_complete\", \"x_mcar\", \"x_mar\", \"x_mnar\"))\n\nres$Continuous\nres$Categorical\n```\n\n\n\n  \n\n  \n\n\n\nPlot of missingness:\n\n```{r}\nmissing_plot(ex_data, dependent = \"y\", explanatory = c(\"x_complete\", \"x_mcar\", \"x_mar\", \"x_mnar\"))\n```\n\n\n\n\nMissingness patterns:\n\n```{r}\nmissing_pattern(ex_data, dependent = \"y\", explanatory = c(\"x_complete\", \"x_mcar\", \"x_mar\", \"x_mnar\"))\n```\n\n  y x_complete x_mcar x_mar x_mnar  \n3 1          1      1     1      1 0\n2 1          1      1     1      0 1\n2 1          1      1     0      1 1\n1 1          1      0     1      1 1\n1 1          1      0     1      0 2\n1 1          1      0     0      1 2\n  0          0      3     3      3 9\n\n\n\n\n\nThere are six missingness pattern on the data.\nCompare:\n\n```{r}\nmissing_pairs(ex_data, dependent = \"y\", explanatory = c(\"x_complete\", \"x_mcar\", \"x_mar\", \"x_mnar\"), position = \"fill\")\n```\n\n\n\n\nAnd lastly, we perform Little’s MCAR tests:\n\nMCAR case:\n\n```{r}\nmcar_test(data.frame(x = ex_data$x_mcar, y = ex_data$y))\n```\n\n\n\n  \n\n\n\nFail to reject null hypothesis, data is MCAR.\nMAR case:\n\n```{r}\nmcar_test(data.frame(x = ex_data$x_mar, y = ex_data$y))\n```\n\n\n\n  \n\n\n\nReject null, data is not MCAR.\nMNAR case:\n\n```{r}\nmcar_test(data.frame(x = ex_data$x_mnar, y = ex_data$y))\n```\n\n\n\n  \n\n\n\nFail to reject null, data is MCAR (which is wrong).\n\nFull source code: https://github.com/mrtkp9993/MyDsProjects/tree/main/MissingData"
  },
  {
    "objectID": "MissingData/Analysis.html#references",
    "href": "MissingData/Analysis.html#references",
    "title": "Don’t impute all missing data: Missing Data Patterns",
    "section": "References",
    "text": "References\n\nEnders, C. (2010). Applied Missing Data Analysis.\nRUBIN, D. B. (1976). Inference and missing data (Vol. 63, Issue 3, pp. 581–592). Oxford University Press (OUP). https://doi.org/10.1093/biomet/63.3.581\nMissing data - Wikipedia. (2016, May 13). https://en.wikipedia.org/wiki/Missing_data\nLittle, R. J. A. (1988). A Test of Missing Completely at Random for Multivariate Data with Missing Values (Vol. 83, Issue 404, pp. 1198–1202). Informa UK Limited. https://doi.org/10.1080/01621459.1988.10478722\nJanz, N. (n.d.). Advanced Handling of Missing Data. Retrieved August 28, 2022, from https://osf.io/updyq/?action=download&version=1\nFox, G., Negrete-Yankelevich, S., & Sosa, V. (2015). Ecological Statistics: Contemporary Theory and Application."
  },
  {
    "objectID": "StockClustering/Analysis.html",
    "href": "StockClustering/Analysis.html",
    "title": "Stock Return and Fundamental Clustering & Portfolio Selection",
    "section": "",
    "text": "We have following features for BIST30 stocks:\nWe need to standardize the features:\nWe will use K-means method. Let’s plot elbow plot:\nWe have four clusters, let’s fit model and see results:\nThere is no negative silhouette score, but second cluster’s score is near to zero. It indicates that seperetion is not well.\nCluster contents:\nLet’s look the mean return statistics of each cluster:\nMean returns will be better if we select stocks from a bigger stock set like BIST100 or all BIST stocks.\nFull source code: https://github.com/mrtkp9993/MyDsProjects/tree/main/StockClustering\n—\nThanks for reading. If you enjoyed, please share it.\nSupport my work from here GitHub Sponsors."
  },
  {
    "objectID": "StockClustering/Analysis.html#references",
    "href": "StockClustering/Analysis.html#references",
    "title": "Stock Return and Fundamental Clustering & Portfolio Selection",
    "section": "References",
    "text": "References\n\nhttps://www.investopedia.com/terms/c/cluster_analysis.asp\nhttps://www.investopedia.com/terms/f/factor-investing.asp"
  },
  {
    "objectID": "TimeSeriesPredictability/Analysis.html",
    "href": "TimeSeriesPredictability/Analysis.html",
    "title": "Don’t Try to Forecast Everything: Predictability of Time Series",
    "section": "",
    "text": "Most of time series analyses start with investigating series, autocorrelation and partial autocorrelation plots. Then one estimates different time series models (like ARIMA, GARCH, State-space models) and performs model checks.\nBut no one asks whether that series is predictable or not.\nWe’ll look at a few handy tools that give more information about our time series.\n—\nThanks for reading. If you enjoyed, please share it.\nSupport my work from here GitHub Sponsors."
  },
  {
    "objectID": "TimeSeriesPredictability/Analysis.html#data",
    "href": "TimeSeriesPredictability/Analysis.html#data",
    "title": "Don’t Try to Forecast Everything: Predictability of Time Series",
    "section": "Data",
    "text": "Data\nWe’ll use some example time series:\n\nMonthly Airline Passenger Numbers 1949-1960 (AirPassengers) [6]\n\n\n\n\n\n\nLevel of Lake Huron 1875-1972 (LakeHuron) [6]\n\n\n\n\n\n\nSimulated time-series data from the Logistic map with chaos [1]\n\n\n\n\n\n\nLet’s look the tools."
  },
  {
    "objectID": "TimeSeriesPredictability/Analysis.html#lyapunov-exponent",
    "href": "TimeSeriesPredictability/Analysis.html#lyapunov-exponent",
    "title": "Don’t Try to Forecast Everything: Predictability of Time Series",
    "section": "Lyapunov Exponent",
    "text": "Lyapunov Exponent\nLyapunov exponent of a dynamical system is a quantity that characterizes the rate of separation of infinitesimally close trajectories. Quantitatively, two trajectories in phase space with initial separation vector \\(\\delta Z_0\\) diverge at a rate given by\n\\[\n|\\delta Z(t)|\\approx e^{\\lambda t}|\\delta Z_0|\n\\]\nwhere \\(\\lambda\\) is the Lyapunov exponent. The rate of separation can be different for different orientations of initial separation vector. Thus, there is a spectrum of Lyapunov exponents. It is common to refer to the largest one as the maximal Lyapunov exponent (MLE), because it determines a notion of predictability for a dynamical system [7].\n\n\nI’ll not go into detail on how to calculate the maximal Lyapunov exponent, we’ll look at practical implications.\nA positive MLE is usually taken as an indication that the system is chaotic [7]."
  },
  {
    "objectID": "TimeSeriesPredictability/Analysis.html#hurst-exponent",
    "href": "TimeSeriesPredictability/Analysis.html#hurst-exponent",
    "title": "Don’t Try to Forecast Everything: Predictability of Time Series",
    "section": "Hurst Exponent",
    "text": "Hurst Exponent\nThe Hurst exponent is referred to as the “index of dependence” or “index of long-range dependence”. It quantifies the relative tendency of a time series either to regress strongly to the mean or to cluster in a direction:\n\nTrending (Persistent) series: If \\(0.5 < H \\leq 1\\) , then series has long-term positive autocorrelation, so a high value in the series will probably be followed by another high value and the future will also tend to be high;\nRandom walk series: if \\(H = 0.5\\), then series is a completely uncorrelated series, so it can go either way (up or down);\nMean-reverting (Anti-persistent) series: if \\(0 \\leq H < 0.5\\), then series has mean-reversion, so a high value in the series will probably be followed by a low value and vice versa [8]."
  },
  {
    "objectID": "TimeSeriesPredictability/Analysis.html#detrended-fluctuation-analysis",
    "href": "TimeSeriesPredictability/Analysis.html#detrended-fluctuation-analysis",
    "title": "Don’t Try to Forecast Everything: Predictability of Time Series",
    "section": "Detrended Fluctuation Analysis",
    "text": "Detrended Fluctuation Analysis\nDFA is a method for determining the statistical self-affinity of a signal. It is the generalization of Hurst exponent, it means [10]:\n\nfor \\(0<\\alpha<0.5\\), then the series is anti-correlated;\nfor \\(\\alpha=0.5\\), then the series is uncorrelated and corresponds to white noise;\nfor \\(0.5<\\alpha<1\\), then the series is correlated;\nfor \\(\\alpha\\approx1\\), then the series corresponds to pink noise;\nfor \\(\\alpha>1\\), then the series is nonstationary and unbounded;\nfor \\(\\alpha\\approx1.5\\), then the series corresponds to Brownian noise."
  },
  {
    "objectID": "TimeSeriesPredictability/Analysis.html#variance-ratio-test",
    "href": "TimeSeriesPredictability/Analysis.html#variance-ratio-test",
    "title": "Don’t Try to Forecast Everything: Predictability of Time Series",
    "section": "Variance Ratio Test",
    "text": "Variance Ratio Test\nThis test is often used to test the hypothesis that a given time series is a collection of i.i.d. observations or that it follows a martingale difference sequence.\nWe will use Chow and Denning’s multiple variance ratio test. There are two tests:\n\nCD1 - Test for i.i.d. series,\nCD2 - Test for uncorrelated series with possible heteroskedasticity.\n\nIf test statistics are bigger than critical values, the null hypothesis is rejected which means the series is not a random walk."
  },
  {
    "objectID": "TimeSeriesPredictability/Analysis.html#statistics-of-the-series",
    "href": "TimeSeriesPredictability/Analysis.html#statistics-of-the-series",
    "title": "Don’t Try to Forecast Everything: Predictability of Time Series",
    "section": "Statistics of the series",
    "text": "Statistics of the series\n\nAirPassengers data\nResults:\n\nLyapunov exponent spectrum:\n\n\nCall:\nLyapunov exponent spectrum \n\nCoefficients:\n             Estimate Std. Error   z value      Pr(>|z|)\nExponent 1 -0.8398548  0.2333552 -28.33887 5.739062e-177\nExponent 2 -1.5136329  0.1937088 -61.52719  0.000000e+00\n---\nProcedure: QR decomposition by bootstrap blocking method \nEmbedding dimension: 2, Time-delay: 1, No. hidden units: 10\nSample size: 129, Block length: 62, No. blocks: 1000\n\n\nThere are two statistically significant exponent estimates. The largest one is -0.84 which is negative, which means the series is not chaotic.\nHurst exponent is 0.8206234; it is bigger than 0.5, so series is trending.\nDFA is estimated as 1.2988566; it is nonstationary and unbounded.\nVariance ratio test:\n\n\n$Holding.Periods\n[1]  2  4  5  8 10 27\n\n$CD1\n[1] 24.48521\n\n$CD2\n[1] 21.22941\n\n$Critical.Values_10_5_1_percent\n[1] 2.378000 2.631038 3.142756\n\n\nBoth of test statistics are bigger than critical values, so the series is not a random walk.\n\n\n\nLakeHuron data\nResults:\n\nLyapunov exponent spectrum:\n\n\nCall:\nLyapunov exponent spectrum \n\nCoefficients:\n             Estimate Std. Error    z value Pr(>|z|)\nExponent 1 -0.2245224 0.03079226  -56.00722        0\nExponent 2 -0.6465142 0.01144893 -433.74968        0\nExponent 3 -0.6696687 0.01006248 -511.18811        0\nExponent 4 -1.6931702 0.02747627 -473.33519        0\n---\nProcedure: QR decomposition by bootstrap blocking method \nEmbedding dimension: 4, Time-delay: 1, No. hidden units: 2\nSample size: 94, Block length: 59, No. blocks: 1000\n\n\nThere are four statistically significant exponent estimates. The largest one is -0.22 which is negative, which means the series is not chaotic.\nHurst exponent is 0.7364948; it is bigger than 0.5, so series is trending.\nDFA is estimated as 1.1128455; it is nonstationary and unbounded.\nVariance ratio test:\n\n\n$Holding.Periods\n[1]  2  4  5  8 10  3\n\n$CD1\n[1] 11.45734\n\n$CD2\n[1] 9.407748\n\n$Critical.Values_10_5_1_percent\n[1] 2.378000 2.631038 3.142756\n\n\nBoth of test statistics are bigger than critical values, so the series is not a random walk.\n\n\n\nSimulated time-series data from the Logistic map with chaos\nResults:\n\nLyapunov exponent spectrum:\n\n\nCall:\nLyapunov exponent spectrum \n\nCoefficients:\n            Estimate Std. Error   z value Pr(>|z|)\nExponent 1 -1.291195  0.1580609 -63.27662        0\n---\nProcedure: QR decomposition by bootstrap blocking method \nEmbedding dimension: 1, Time-delay: 1, No. hidden units: 2\nSample size: 99, Block length: 60, No. blocks: 1000\n\n\nThere is one statistically significant exponent estimate, -1.29 which is negative, which means the series is not chaotic which is a questionable result.\nHurst exponent is 0.6255664; it is bigger than 0.5, so series is trending.\nDFA is estimated as 0.758476; it is correlated.\nVariance ratio test:\n\n\n$Holding.Periods\n[1]  2  4  5  8 10 10\n\n$CD1\n[1] 1.193817\n\n$CD2\n[1] 1.295116\n\n$Critical.Values_10_5_1_percent\n[1] 2.378000 2.631038 3.142756\n\n\nBoth of test statistics are smaller than critical values, so the series is a random walk.\n\nFull source code: https://github.com/mrtkp9993/MyDsProjects/tree/main/TimeSeriesPredictability"
  },
  {
    "objectID": "TimeSeriesPredictability/Analysis.html#references",
    "href": "TimeSeriesPredictability/Analysis.html#references",
    "title": "Don’t Try to Forecast Everything: Predictability of Time Series",
    "section": "References",
    "text": "References\n\nDChaos, https://cran.r-project.org/web/packages/DChaos/index.html\nstatcomp, https://cran.r-project.org/web/packages/statcomp/index.html\npracma, https://cran.r-project.org/web/packages/pracma/index.html\ntseriesChaos, https://cran.r-project.org/web/packages/tseriesChaos/index.html\nDaniel F. McCaffrey , Stephen Ellner , A. Ronald Gallant & Douglas W. Nychka (1992) Estimating the Lyapunov Exponent of a Chaotic System with Nonparametric Regression, Journal of the American Statistical Association, 87:419, 682-695\nboot, https://www.rdocumentation.org/packages/boot/versions/1.3-28/topics/boot.\nContributors to Wikimedia projects. “Lyapunov exponent - Wikipedia.” 7 July 2022, https://en.wikipedia.org/w/index.php?title=Lyapunov_exponent&oldid=1096875011.\nContributors to Wikimedia projects. “Hurst exponent - Wikipedia.” 12 June 2022, https://en.wikipedia.org/w/index.php?title=Hurst_exponent&oldid=1092814465.\nDFA, https://cran.r-project.org/package=DFA\nContributors to Wikimedia projects. “Detrended fluctuation analysis - Wikipedia.” 19 June 2022, https://en.wikipedia.org/w/index.php?title=Detrended_fluctuation_analysis&oldid=1093832537.\nnonlinearTseries, https://cran.r-project.org/web/packages/nonlinearTseries/index.html."
  }
]