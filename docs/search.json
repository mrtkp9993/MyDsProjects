[
  {
    "objectID": "UncertaintyQuantification/Analysis.html",
    "href": "UncertaintyQuantification/Analysis.html",
    "title": "Uncertainty Quantification with Polynomial Chaos",
    "section": "",
    "text": "Source: \\(^4\\)\n\n\nAccording to \\(^2\\), uncertainty quantification is defined as\n\nThe process of quantifying uncertainties associated with model calculations of true, physical QOIs, with the goals of accounting for all sources of uncertainty and quantifying the contributions of specific sources to the overall uncertainty.\n\nand answers the question\n\nHow do the various sources of error and uncertainty feed into uncertainty in the model-based prediction of the quantities of interest?"
  },
  {
    "objectID": "UncertaintyQuantification/Analysis.html#logistic-growth-model-example",
    "href": "UncertaintyQuantification/Analysis.html#logistic-growth-model-example",
    "title": "Uncertainty Quantification with Polynomial Chaos",
    "section": "Logistic Growth Model Example",
    "text": "Logistic Growth Model Example\nLogistic growth model is defined as\n\\[\n\\frac{dX}{dt}=rX(1-\\frac{X}{K})\n\\]\nwhere \\(r\\) is the growth rate and \\(K\\) is the population capacity (horizontal asymptote).\nLet’s define the model and visualize it for some parameters and the initial condition \\(X_0=50\\):\n\nt = numpy.linspace(0, 10, 100)\nx0 = 50\n\ndef logistic_model(x, t, r, K):\n    return r * x * (1 - x / K)\n  \nfig, axs = plt.subplots(2,2)\nfig.suptitle('Logistic model with different params')\n\naxs = axs.ravel()\n\nfor i, params in enumerate([(0.7, 60), (1.1, 60), (0.7, 300), (1.1, 300)]):\n  sol = odeint(logistic_model, x0, t, args=params)\n  axs[i].plot(t, sol[:, 0], 'b', label='x(t)')\n  axs[i].legend(loc='best')\n  axs[i].set_xlabel('t')\n  axs[i].set_ylabel('x')\n  axs[i].grid()\n  axs[i].plot()\n    \nplt.show()\n\n\n\n\nNow let’s assume that we have uncertainties over our parameters and assume that\n\\[\n\\begin{align*}\nr &\\sim \\text{Log-Normal}(1, 0.1)\\\\\nK &\\sim \\text{Uniform}(100, 200)\n\\end{align*}\n\\] Let’s define our joint distribution:\n\nrdist= chaospy.LogNormal(1, 0.1)\nKdist = chaospy.Uniform(100, 200)\njoint = chaospy.J(rdist, Kdist)\n\ngrid = numpy.mgrid[joint.lower[0]:joint.upper[0]+1, joint.lower[1]:joint.upper[1]+1]\ncontour = plt.contourf(grid[0], grid[1], joint.pdf(grid), 50)\nplt.scatter(*joint.sample(50, seed=1234))\nplt.xlim(joint.lower[0], joint.upper[0])\nplt.ylim(joint.lower[1], joint.upper[1])\nplt.show()\n\n\n\n\nGenerate expension, sample the joint distribution, evaluate model at these points and plot:\n\nexpansion = chaospy.generate_expansion(order=3, dist=joint)\n\n# and sample the joint distribution\nsamples = joint.sample(1000, rule=\"sobol\")\n\n# and evulate solver at these samples\nevaluations = numpy.array([odeint(logistic_model, x0, t, args=(sample[0], sample[1])) for sample in samples.T])\n\n# and plot\nplt.plot(t, evaluations[:,:,0].T, alpha=0.1)\nplt.show()\n\n\n\n\nCreate polynomial approximation:\n\napprox_solver = chaospy.fit_regression(expansion, samples, evaluations)\n\nCalculate mean and deviance and plot:\n\nexpected = chaospy.E(approx_solver, joint)\ndeviation = chaospy.Std(approx_solver, joint)\n\nplt.fill_between(t, expected[:,0]-2*deviation[:,0], expected[:,0]+2*deviation[:,0], alpha=0.4)\nplt.plot(t, expected[:,0])\nplt.show()\n\n\n\n\nFull source code: https://github.com/mrtkp9993/MyDsProjects/tree/main/UncertaintyQuantification"
  },
  {
    "objectID": "TimeSeriesClassification/Analysis.html",
    "href": "TimeSeriesClassification/Analysis.html",
    "title": "Time Series Classification with Random Forests",
    "section": "",
    "text": "First, let’s look the methodology behind the idea.\nSuppose that \\(N\\) training time series examples \\(\\{e_1,e_2,\\ldots,e_N\\}\\) and the corresponding class labels \\(\\{y_1,\\ldots,y_N\\},\\quad y_i\\in\\{1,2,\\ldots,C\\}\\) where \\(C\\) is the class count, are given. The task is to predict the class labels for test examples. Here, for simplicity, we assume the values of time series are measured at equally-spaced intervals and training and test time series examples are of the same length \\(M\\) \\(^1\\).\n\n\n\nSource: https://www.sciencedirect.com/science/article/pii/B9780128119686000097\n\n\nTime series classification methods can be divided into two categories: Instance-based and feature-based. Instance-based methods like nearest-neighbor classifiers with Euclidean distance (NNEuclidean) or dynamic time warping (NNDTW) try to classify test examples based on its similarity to the training examples \\(^1\\).\nFeature-based methods build models on temporal features like \\(^3\\):\n\nSingular Value Decomposition (SVD),\nDiscrete Fourier Transform (DFT),\nCoefficients of the decomposition into Chebysev Polynominals,\nDiscrete Wavelet Transform (DWT),\nPiecewise Linear Approximation,\nARMA coefficients,\nSymbolic representations like Symbolic Aggregate approXimation (SAX)."
  },
  {
    "objectID": "TimeSeriesClassification/Analysis.html#atrial-fibrillation",
    "href": "TimeSeriesClassification/Analysis.html#atrial-fibrillation",
    "title": "Time Series Classification with Random Forests",
    "section": "Atrial Fibrillation",
    "text": "Atrial Fibrillation\n\nThis is a physionet dataset of two-channel ECG recordings has been created from data used in the Computers in Cardiology Challenge 2004, an open competition with the goal of developing automated methods for predicting spontaneous termination of atrial fibrillation (AF). The raw instances were 5 second segments of atrial fibrillation, containing two ECG signals, each sampled at 128 samples per second. The Multivate data organises these channels such that each is one dimension. The class labels are: n, s and t. class n is described as a non termination artiral fibrilation(that is, it did not terminate for at least one hour after the original recording of the data). class s is described as an atrial fibrilation that self terminates at least one minuet after the recording process. class t is descirbed as terminating immediatly, that is within one second of the recording ending. PhysioNet Reference: Goldberger AL, Amaral LAN, Glass L, Hausdorff JM, Ivanov PCh, Mark RG, Mietus JE, Moody GB, Peng CK, Stanley HE. PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals. Circulation 101(23):e215-e220 [Circulation Electronic Pages; (Link Here) 2000 (June 13). PMID: 10851218; doi: 10.1161/01.CIR.101.23.e215 Publication: Moody GB. Spontaneous Termination of Atrial Fibrillation: A Challenge from PhysioNet and Computers in Cardiology 2004. Computers in Cardiology 31:101-104 (2004) \\(^5\\).\n\nLet’s read dataset:\n\nX_tr, y_tr = load_from_tsfile_to_dataframe(\"data/AtrialFibrillation_TRAIN.ts\")\nX_ts, y_ts = load_from_tsfile_to_dataframe(\"data/AtrialFibrillation_TEST.ts\")\n\nTake a look:\n\nMarkdown(tabulate(\n  X_tr.head(1), \n  headers=X_tr.columns\n))\n\n\n\n\n\ndim_0\ndim_1\n\n\n\n\n0\n0 -0.34086\n0 0.14820\n\n\n\n1 -0.38038\n1 0.13338\n\n\n\n2 -0.34580\n2 0.10868\n\n\n\n3 -0.36556\n3 0.09386\n\n\n\n4 -0.34580\n4 0.07410\n\n\n\n…\n…\n\n\n\n635 -0.04446\n635 -0.03458\n\n\n\n636 -0.04940\n636 -0.05928\n\n\n\n637 -0.02964\n637 -0.06916\n\n\n\n638 -0.01976\n638 -0.06916\n\n\n\n639 0.00000\n639 -0.07410\n\n\n\nLength: 640, dtype: float64\nLength: 640, dtype: float64\n\n\n\n\n\n\nMarkdown(tabulate(\n  y_tr, \n  headers=\"Labels\"\n))\n\nL\n\nn n n n n s s s s s t t t t t\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=1)\n\nX_tr.iloc[0,0].plot(ax=axes[0])\nX_tr.iloc[0,1].plot(ax=axes[1])\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.88)\nfig.suptitle(\"non termination artiral fibrilation\")\nplt.show()\n\n\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=1)\n\nX_tr.iloc[6,0].plot(ax=axes[0])\nX_tr.iloc[6,1].plot(ax=axes[1])\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.88)\nfig.suptitle(\"an atrial fibrilation that self terminates\")\nplt.show()\n\n\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=1)\n\nX_tr.iloc[12,0].plot(ax=axes[0])\nX_tr.iloc[12,1].plot(ax=axes[1])\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.88)\nfig.suptitle(\"terminating immediatly\")\nplt.show()\n\n\n\n\nOur data is multivariate. We’ll use ColumnConcatenator which concatenates each dimension and converts multivariate time series to univariate time series.\n\ncc = ColumnConcatenator()\nX_tr = cc.fit_transform(X_tr)\nX_ts = cc.fit_transform(X_ts)\n\nLet’s train classifier:\n\nclf = TimeSeriesForestClassifier(min_interval=200, n_estimators=10000, n_jobs=-1, random_state=1234)\nclf.fit(X_tr, y_tr)\n\nTimeSeriesForestClassifier(min_interval=200, n_estimators=10000, n_jobs=-1,\n                           random_state=1234)\n\n\nPredict test data and check accuracy:\n\ny_pr = clf.predict(X_ts)\nprint(classification_report(y_ts, y_pr))\n\n              precision    recall  f1-score   support\n\n           n       1.00      0.40      0.57         5\n           s       0.50      0.60      0.55         5\n           t       0.29      0.40      0.33         5\n\n    accuracy                           0.47        15\n   macro avg       0.60      0.47      0.48        15\nweighted avg       0.60      0.47      0.48        15\n\n\n\nVisualize confusion matrix:\n\ncm = confusion_matrix(y_ts, y_pr, labels=['n', 's', 't'])\nax = sn.heatmap(pd.DataFrame(cm), annot=True, square=True, cbar=False, fmt='g')\nplt.xlabel(\"Predicted\") \nplt.ylabel(\"Actual\") \nax.invert_yaxis()\nplt.show()\n\n\n\n\nFull source code: https://github.com/mrtkp9993/MyDsProjects/tree/main/TimeSeriesClassification"
  },
  {
    "objectID": "StockClusteringDiverse/Analysis.html",
    "href": "StockClusteringDiverse/Analysis.html",
    "title": "Build Diversified Portfolio with Machine Learning: Clustering method for stock selection",
    "section": "",
    "text": "Introduction\nLast time, we analyzed similar stocks in XU30 index and calculated each cluster’s mean return and risk.\nSimilar method can be used to diversify portfolio and minimizing the risk: we’ll again apply clustering methods to determine different subsets of stocks in the XU100 market. And we select one stock from each cluster and calculate their return and risk and try to beat the market.\nNote: We assume that the index components have not changed over the time. For a more realistic calculation, the stocks added to and excluded from the index should also be taken into account.\n\n\n\nSource: https://www.pexels.com/tr-tr/fotograf/borsa-kurulu-210607/\n\n\n\n\nData\nRead data:\n\nlibrary(cluster)\nlibrary(factoextra)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(NbClust)\nlibrary(quantmod)\nlibrary(tibble)\nlibrary(tidyquant)\nlibrary(tidyr)\n\nset.seed(1234)\n\n# Delete the comment for downloading the data\n# stock_list &lt;- read.table(\"bist100_stocks.txt\") \n# stock_ohlc &lt;- stock_list$V1 |&gt; tq_get() \n# saveRDS(stock_ohlc, file=\"stock_data.rds\")\nstock_ohlc &lt;- readRDS(\"stock_data.rds\")\n\n# Fix the index data (XU100.IS)\nstock_ohlc$date &lt;- as.Date(stock_ohlc$date, format = \"%Y-%m-%d\")\nstock_ohlc &lt;- stock_ohlc %&gt;% mutate(\n  adjusted = ifelse(\n    symbol == 'XU100.IS' & date &lt; \"2020-07-27\", adjusted / 100, adjusted)\n)\n\nVisualize annual returns of 5 stocks which has most volume:\n\ntop_5_by_vol &lt;- stock_ohlc %&gt;% \n  filter(symbol != \"XU100.IS\") %&gt;%\n  group_by(symbol) %&gt;% \n  summarise(Volume = sum(volume)) %&gt;% \n  top_n(5)\n\nstock_ohlc %&gt;% \n  filter(symbol %in% top_5_by_vol$symbol) %&gt;% \n  group_by(symbol) %&gt;% \n  tq_transmute(select     = adjusted, \n               mutate_fun = periodReturn, \n               period     = \"yearly\", \n               col_rename = \"yearly.returns\") %&gt;%\n  ggplot(aes(x = year(date), y = yearly.returns, fill = symbol)) +\n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  labs(title = \"5 Most Active XU100 Stocks\", \n       y = \"Returns\", x = \"\", color = \"\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_tq() +\n  scale_fill_tq()\n\n\n\n\nCalculate daily returns for each stock and the market:\n\ndaily_returns &lt;- stock_ohlc %&gt;% \n  group_by(symbol) %&gt;%\n  tq_transmute(select = adjusted,\n               mutate_fun = periodReturn,\n               type = \"log\",\n               period = \"daily\",\n               col_rename = \"daily.returns\")\n\nPivot the data:\n\ndaily_returns_p &lt;- daily_returns %&gt;% \n  pivot_wider(names_from = symbol, values_from = daily.returns)\n\nCheck the NA counts:\n\ncolSums(is.na(daily_returns_p))\n\n    date XU100.IS AGHOL.IS AKBNK.IS AKCNS.IS  AKSA.IS AKSEN.IS ALGYO.IS \n       0       80        0        0        0        0        0        0 \nALARK.IS ALBRK.IS ALKIM.IS AEFES.IS ARCLK.IS ARDYZ.IS ASELS.IS AYDEM.IS \n       0        0        0        0        0     2111        0     2421 \nAYGAZ.IS  BERA.IS BIMAS.IS BIOEN.IS BRISA.IS CCOLA.IS CANTE.IS CEMTS.IS \n       0      234        0     2763        0        0     2421        0 \nCIMSA.IS  DEVA.IS DOHOL.IS  DOAS.IS EGEEN.IS ECILC.IS EKGYO.IS ENJSA.IS \n       0        0        0        0        0        0        0     1593 \nENKAI.IS ERBOS.IS EREGL.IS  ESEN.IS FROTO.IS GLYHO.IS GOZDE.IS GUBRF.IS \n       0        0        0     2763        0        0        0        0 \nSAHOL.IS HLGYO.IS HEKTS.IS INDES.IS ISDMR.IS ISFIN.IS ISGYO.IS ISMEN.IS \n       0      312        0        0     1105        0        0        0 \nIZMDC.IS KRDMD.IS KARSN.IS KARTN.IS KERVT.IS KRVGD.IS KCHOL.IS KORDS.IS \n       0        0        0        0        0     2763        0        0 \nKOZAL.IS KOZAA.IS  LOGO.IS  MAVI.IS MGROS.IS MPARK.IS NETAS.IS  ODAS.IS \n       0        0        0     1423        0     1596        0      361 \nOTKAR.IS OYAKC.IS PARSN.IS PGSUS.IS PETKM.IS QUAGR.IS SARKY.IS  SASA.IS \n       0        0        0      405        0     2407        0        0 \nSELEC.IS SKBNK.IS  SOKM.IS TAVHL.IS TKFEN.IS TKNSA.IS TOASO.IS TRGYO.IS \n       0        0     1664        0        0       98        0        0 \nTRILC.IS TCELL.IS  TMSN.IS TUPRS.IS THYAO.IS TTKOM.IS TTRAK.IS GARAN.IS \n    2763        0      406        0        0        0        0        0 \nHALKB.IS ISCTR.IS  TSKB.IS TURSG.IS  SISE.IS VAKBN.IS ULKER.IS VERUS.IS \n       0        0        0        0        0        0        0      493 \nVESBE.IS VESTL.IS YKBNK.IS YATAS.IS ZRGYO.IS ZOREN.IS \n       0        0        0        0     2763        0 \n\n\nSome stocks have only one day data, 30% or more missing data. We discard them and subset the data:\n\ndaily_returns_p &lt;- daily_returns_p %&gt;%\n  select(where(~ sum(is.na(.x)) &lt; 0.3 * 2764))\n\nindx &lt;- complete.cases(daily_returns_p)\ndaily_returns_p &lt;- daily_returns_p[indx, ]\n\nCheck NA count again and date continuity:\n\nprint(sum(colSums(is.na(daily_returns_p))))\n\n[1] 0\n\nprint(daily_returns_p %&gt;% select(date) %&gt;% mutate(date_diff = date - lag(date)) %&gt;% filter(date_diff &gt; 3))\n\n# A tibble: 37 × 2\n   date       date_diff\n   &lt;date&gt;     &lt;drtn&gt;   \n 1 2014-05-20 4 days   \n 2 2014-07-31 6 days   \n 3 2014-10-08 5 days   \n 4 2015-05-04 4 days   \n 5 2015-07-20 4 days   \n 6 2015-09-28 5 days   \n 7 2016-01-04 4 days   \n 8 2016-07-08 4 days   \n 9 2016-09-16 7 days   \n10 2017-05-02 4 days   \n# … with 27 more rows\n\n\nI’ll omit the gaps for sake of simplicity. Finally, split the market and stocks data:\n\nmarket_return &lt;- tibble(date=daily_returns_p$date, \n                        daily.return=daily_returns_p$XU100.IS)\nstock_returns &lt;- tibble(date=daily_returns_p$date,\n                        daily_returns_p[,!(colnames(daily_returns_p) %in% c(\"date\", \"XU100.IS\"))])\n\n\n\nClustering\nLet’s standardize the data:\n\nstock_returns_scaled &lt;- stock_returns %&gt;% select(!date) %&gt;% mutate_each(funs(scale))\nstock_returns_scaled[1:5, 1:5]\n\n\n\n  \n\n\n\nTranspose the data:\n\nstock_returns_scaled &lt;- data.frame(row.names = names(stock_returns_scaled), t(stock_returns_scaled))\n\nFind optimum number of clusters for k-means:\n\nnb &lt;- NbClust(stock_returns_scaled, method = \"kmeans\", min.nc = 3, max.nc = 7, index=\"gap\")\nnb\n\n$All.index\n      3       4       5       6       7 \n-0.9453 -1.4654 -1.6998 -2.1534 -2.6769 \n\n$All.CriticalValues\n     3      4      5      6      7 \n0.5352 0.2520 0.4748 0.5507 0.1737 \n\n$Best.nc\nNumber_clusters     Value_Index \n         3.0000         -0.9453 \n\n$Best.partition\nAGHOL.IS AKBNK.IS AKCNS.IS  AKSA.IS AKSEN.IS ALGYO.IS ALARK.IS ALBRK.IS \n       1        3        1        1        1        1        1        1 \nALKIM.IS AEFES.IS ARCLK.IS ASELS.IS AYGAZ.IS  BERA.IS BIMAS.IS BRISA.IS \n       1        3        3        1        1        1        3        1 \nCCOLA.IS CEMTS.IS CIMSA.IS  DEVA.IS DOHOL.IS  DOAS.IS EGEEN.IS ECILC.IS \n       3        1        1        2        1        1        1        2 \nEKGYO.IS ENKAI.IS ERBOS.IS EREGL.IS FROTO.IS GLYHO.IS GOZDE.IS GUBRF.IS \n       3        1        1        3        3        1        1        1 \nSAHOL.IS HLGYO.IS HEKTS.IS INDES.IS ISFIN.IS ISGYO.IS ISMEN.IS IZMDC.IS \n       3        1        1        1        1        1        1        1 \nKRDMD.IS KARSN.IS KARTN.IS KERVT.IS KCHOL.IS KORDS.IS KOZAL.IS KOZAA.IS \n       3        1        1        1        3        1        2        2 \n LOGO.IS MGROS.IS NETAS.IS  ODAS.IS OTKAR.IS OYAKC.IS PARSN.IS PGSUS.IS \n       1        3        1        1        1        1        1        3 \nPETKM.IS SARKY.IS  SASA.IS SELEC.IS SKBNK.IS TAVHL.IS TKFEN.IS TKNSA.IS \n       3        1        1        2        1        3        3        1 \nTOASO.IS TRGYO.IS TCELL.IS  TMSN.IS TUPRS.IS THYAO.IS TTKOM.IS TTRAK.IS \n       3        1        3        1        3        3        3        1 \nGARAN.IS HALKB.IS ISCTR.IS  TSKB.IS TURSG.IS  SISE.IS VAKBN.IS ULKER.IS \n       3        3        3        3        1        3        3        3 \nVERUS.IS VESBE.IS VESTL.IS YKBNK.IS YATAS.IS ZOREN.IS \n       1        1        1        3        1        1 \n\n\nOptimal cluster count is three, let’s fit and look cluster contents:\n\nkm_model &lt;- kmeans(stock_returns_scaled, 3)\nfviz_cluster(object = km_model,\n             data = stock_returns_scaled,\n             ellipse.type = \"norm\",\n             geom = \"text\",\n             palette = \"jco\",\n             main = \"\",\n             ggtheme = theme_minimal())\n\n\n\n\n\n\nMarket Performance and Portfolio Construction\nWe’ve XU100 daily returns as baseline.\nLet’s select one stock from each cluster randomly two times and construct two different portfolios:\n\n# Select two stocks from each cluster randomly\n# data.frame(stock=names(km_model$cluster),\n#            cluster=km_model$cluster) %&gt;%\n#   group_by(km_model$cluster) %&gt;% sample_n(2)\n\nIn my run, it selected EGEEN and ECILC from cluster 1, FROTO and THYAO from cluster 2 and AKSA and TTRAK from cluster 3. Construct two different portfolios with them:\n\nstocks &lt;- c(\"EGEEN.IS\", \"ECILC.IS\", \"FROTO.IS\", \"THYAO.IS\", \"AKSA.IS\", \"TTRAK.IS\")\n\nweights &lt;- c(\n    0.34, 0.33, 0.33, 0.0, 0.0, 0.0,\n    0.0, 0.0, 0.0, 0.34, 0.33, 0.33\n)\nweights_table &lt;-  tibble(stocks) %&gt;%\n    tq_repeat_df(n = 2) %&gt;%\n    bind_cols(tibble(weights)) %&gt;%\n    group_by(portfolio)\n\nportfolio_returns &lt;- daily_returns %&gt;%\n    filter(date &gt;= daily_returns_p$date[1]) %&gt;%\n    tq_repeat_df(n = 2) %&gt;%\n    tq_portfolio(assets_col  = symbol, \n                 returns_col = daily.returns, \n                 weights     = weights_table, \n                 col_rename  = \"portfolio.returns\")\nhead(portfolio_returns)\n\n\n\n  \n\n\n\nMerge daily return data:\n\ndaily_returns_portfolios &lt;- left_join(portfolio_returns, market_return, , by=\"date\")\ndaily_returns_portfolios\n\n\n\n  \n\n\n\nCalculate performances, risks, CAPM table:\n\ndaily_returns_portfolios %&gt;%\n    tq_performance(Ra = portfolio.returns, Rb = daily.return, performance_fun = table.CAPM)\n\n\n\n  \n\n\ndaily_returns_portfolios %&gt;%\n    tq_performance(Ra = portfolio.returns, Rb = daily.return, performance_fun = table.DownsideRisk)\n\n\n\n  \n\n\ndaily_returns_portfolios %&gt;%\n    tq_performance(Ra = portfolio.returns, Rb = NULL, performance_fun = table.AnnualizedReturns)\n\n\n\n  \n\n\n\nVisualize portfolio growths with 1000 TRY initial capital, let’s add the index XU100 to the graph:\n\nstocks &lt;- c(\"XU100.IS\", \"EGEEN.IS\", \"ECILC.IS\", \"FROTO.IS\", \"THYAO.IS\", \"AKSA.IS\", \"TTRAK.IS\")\n\nweights &lt;- c(\n    1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n    0.0, 0.34, 0.33, 0.33, 0.0, 0.0, 0.0,\n    0.0, 0.0, 0.0, 0.0, 0.34, 0.33, 0.33\n)\nweights_table &lt;-  tibble(stocks) %&gt;%\n    tq_repeat_df(n = 3) %&gt;%\n    bind_cols(tibble(weights)) %&gt;%\n    group_by(portfolio)\n\nportfolio_returns &lt;- daily_returns %&gt;%\n    filter(date &gt;= daily_returns_p$date[1]) %&gt;%\n    tq_repeat_df(n = 3) %&gt;%\n    tq_portfolio(assets_col  = symbol, \n                 returns_col = daily.returns, \n                 weights     = weights_table, \n                 col_rename  = \"portfolio.returns\")\n\nportfolio_growth_daily &lt;- daily_returns %&gt;%\n    filter(date &gt;= daily_returns_p$date[1]) %&gt;%\n    tq_repeat_df(n = 3) %&gt;%\n    tq_portfolio(assets_col  = symbol, \n                 returns_col = daily.returns, \n                 weights     = weights_table, \n                 col_rename   = \"investment.growth\",\n                 wealth.index = TRUE) %&gt;%\n    mutate(investment.growth = investment.growth * 1000)\n\nportfolio_growth_daily %&gt;%\n    ggplot(aes(x = date, y = investment.growth, color = factor(portfolio))) +\n    geom_line(size = 2) +\n    labs(title = \"Portfolio Growth\",\n         x = \"\", y = \"Portfolio Value\",\n         color = \"Portfolio\") +\n    geom_smooth(method = \"loess\") +\n    theme_tq() +\n    scale_color_tq() +\n    scale_y_continuous()\n\n\n\n\nFull source code: https://github.com/mrtkp9993/MyDsProjects/tree/main/StockClusteringDiverse\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nCitationBibTeX citation:@online{koptur2022,\n  author = {Koptur, Murat},\n  title = {Build {Diversified} {Portfolio} with {Machine} {Learning:}\n    {Clustering} Method for Stock Selection},\n  date = {2022-09-18},\n  url = {https://www.muratkoptur.com/MyDsProjects/StockClusteringDiverse/Analysis.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKoptur, Murat. 2022. “Build Diversified Portfolio with Machine\nLearning: Clustering Method for Stock Selection.” September 18,\n2022. https://www.muratkoptur.com/MyDsProjects/StockClusteringDiverse/Analysis.html."
  },
  {
    "objectID": "MLE/Analysis.html",
    "href": "MLE/Analysis.html",
    "title": "Under the hood: Maximum Likelihood Estimation (MLE)",
    "section": "",
    "text": "Source: \\(^1\\)\n\n\nMaximum likelihood is a method for constructing an estimator for an unknown parameter \\(\\theta\\), which was introduced by Fisher in 1912. Method can be applied in most problems and often gives a reasonable estimator for \\(\\theta\\), and if sample size is big, the method will give an excellent estimator of \\(\\theta\\). For these reasons, the method is most widely used estimation method in statistics \\(^2\\).\nMaximum likelihood estimation can be described as follow: suppose that we observed \\(X_1,\\cdots,X_n\\) from a distribution \\(f(x|\\theta)\\). Then we define the likelihood function (the joint probability of the observed data viewed as a function of parameters of the statistical model) as\n\\[L(\\theta)=f(x_1,\\ldots,x_n|\\theta)=f(x_1|\\theta)\\cdots f(x_n|\\theta)\\]\nSince \\(\\text{log}\\) is a monotonic increasing function, maximizing \\(L(\\theta)\\) is equivalent to maximizing \\(\\text{log}L(\\theta)\\). Then we define the log-likelihood function as\n\\[l(\\theta)=\\text{log}L(\\theta)=\\text{log}\\prod_{i=1}^n f(X_i|\\theta)=\\sum_{i=1}^n\\text{log}f(X_i|\\theta)\\]\nMaximizing \\(l(\\theta)\\) with respect to \\(\\theta\\) will give us the MLE \\(^{2,3}\\):\n\\[\\hat{\\theta}=\\mathop{\\mathrm{argmax}}_{\\theta\\in\\Theta}l(\\theta)\\]"
  },
  {
    "objectID": "MLE/Analysis.html#estimate-the-distribution-parameters",
    "href": "MLE/Analysis.html#estimate-the-distribution-parameters",
    "title": "Under the hood: Maximum Likelihood Estimation (MLE)",
    "section": "Estimate the distribution parameters",
    "text": "Estimate the distribution parameters\nAssume that we observed \\(15\\) different \\(X_i\\) values from Poisson distribution with unknown \\(\\lambda\\) (we simulate the data, so we know the true value of parameter \\(\\lambda = 3\\)):\n\nobs_pois &lt;- rpois(15, 3)\n\nplot(obs_pois)\n\n\n\nhist(obs_pois)\n\n\n\n\nSince we know that data is distributed as Poisson, we can write the likelihood and log-likelihood functions as:\n\\[\nL(\\lambda)=\\prod_{i=1}^{n}\\exp(-\\lambda)\\frac{1}{x_i!}\\lambda^{x_i}\n\\]\n\\[\nl(\\lambda)=-n\\lambda-\\sum_{i=1}^n\\ln(x_i!)+\\ln(\\lambda)\\sum_{i=1}^nx_i\n\\]\nLet’s estimate:\n\nloglikMLEpois &lt;- function(params, x_data) {\n  sum(dpois(x_data, lambda = params[1], log = TRUE))\n}\n\nestPois &lt;- optim(c(lambda = 1), fn = loglikMLEpois, x_data = obs_pois, control = list(fnscale=-1), method=\"BFGS\")\n\nround(estPois$par, 4)\n\nlambda \n2.9333 \n\n\nWe estimated \\(\\hat{\\lambda}=\\) 2.9333 which is a good estimation.\nPlot the log-likelihood function:\n\nlogLikValuesPois &lt;- expand.grid(lambda=seq(0, 10, 1))\nlogLikValuesPois$LL &lt;- apply(logLikValuesPois, 1, loglikMLEpois, x_data=obs_pois)\n\nplot(logLikValuesPois$lambda, logLikValuesPois$LL, type=\"o\")"
  },
  {
    "objectID": "MLE/Analysis.html#estimate-the-linear-regression-model-parameters",
    "href": "MLE/Analysis.html#estimate-the-linear-regression-model-parameters",
    "title": "Under the hood: Maximum Likelihood Estimation (MLE)",
    "section": "Estimate the linear regression model parameters",
    "text": "Estimate the linear regression model parameters\nSuppose that we observed 10 different \\((x_i, y_i), \\quad i=1,\\ldots,10\\) samples. For this, we generate some random data (Our data generation process is \\(y = 2x-1+\\epsilon\\)):\n\nx &lt;- seq(1, 10, 1)\ny &lt;- 2 * x - 1 + rnorm(10)\n\ntibble(x, y)\n\n\n\n  \n\n\n\nPlot of our generated data:\n\nplot(x, y, type=\"o\")\n\n\n\n\nWe simulated the data, so we know the true parameter values, \\(\\theta=(\\theta_0, \\theta_1)=(-1, 2)\\). Let’s try to estimate them with MLE.\nIf we assume that errors has the Normal distribution with mean zero and standard deviation one and error variance is constant, we get\n\\[\ny~\\sim N(\\theta_0+\\theta_1x,\\sigma^2)\n\\]\nSo we can write the conditional distribution as:\n\\[\nf(y|x,\\theta_0,\\theta_1,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}\\frac{(y-(\\theta_1x+\\theta_0))^2}{\\sigma^2}}\n\\]\nThen we get following likelihood and log-likelihood functions:\n\\[\nL(\\theta_0,\\theta_1,\\sigma^2)=\\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\prod_{(x,y)\\in X}e^\\frac{-(y-(\\theta_1 x + \\theta_0))^2}{2\\sigma^2}\n\\]\n\\[\nl(\\theta_0,\\theta_1,\\sigma^2)=-\\frac{n}{2}\\log 2\\pi-n\\log \\sigma-\\frac{1}{\\sigma^2}\\sum_{i=1}^n(y_i-(\\theta_1x_i+\\theta_0))^2\n\\]\nLet’s define log-likelihood function in R:\n\nloglikMLEreg &lt;- function(params, y_data) {\n  x_data = x\n  sum(dnorm(y_data, mean = params[1] + params[2] * x_data, sd = params[3], log = TRUE))\n}\n\nMaximize:\n\nestReg &lt;- optim(c(intercept = 0, slope = 1, sigma = 1), fn = loglikMLEreg,\n                y_data = y, control = list(fnscale=-1)\n                )\n\nround(estReg$par, 4)\n\nintercept     slope     sigma \n  -1.0947    1.9684    1.0153 \n\n\nOur estimation is \\(\\hat{\\theta}=(\\)-1.0946921, 1.9683684\\()\\). We know that true values are \\(\\theta=(-1, 2)\\) so this estimation is a pretty good.\nLastly, visualize our log-likelihood function with constant sigma:\n\nlibrary(reshape2)\nlibrary(plotly)\n\nlogLikValues &lt;- expand.grid(theta_0=seq(-5, 5, 1), theta_1=seq(-5, 5, 1))\nlogLikValues$sigma &lt;- 1\nlogLikValues$LL &lt;- apply(logLikValues, 1, loglikMLEreg, y_data=y)\n\nplot_matrix &lt;- t(acast(logLikValues, theta_0~theta_1, value.var=\"LL\"))\n\nplot_ly(\n      x = as.numeric(colnames(plot_matrix)), \n      y = as.numeric(rownames(plot_matrix)), \n      z = plot_matrix\n    ) %&gt;% \n  add_surface() %&gt;%\n    layout(\n    scene = list(\n      xaxis = list(title = \"theta_0\"),\n      yaxis = list(title = \"theta_1\"),\n      zaxis = list(title = \"LL\")\n    ))\n\n\n\n\n\nFull source code: https://github.com/mrtkp9993/MyDsProjects/tree/main/MLE"
  },
  {
    "objectID": "LotkaVolterraOptimalControl/Analysis.html",
    "href": "LotkaVolterraOptimalControl/Analysis.html",
    "title": "Applying Optimal Control to Fishing",
    "section": "",
    "text": "Optimal control theory is a field of control theory that focuses on identifying the best way to control a dynamic system within a given time frame to achieve an optimized objective function while accounting for any constraints or limitations on the system. As we can see from this definition, an optimal control problem has three components:\nIn this text, we will not examine the existince of admissible controls, we will just assume that admissible controls are exists for problems."
  },
  {
    "objectID": "LotkaVolterraOptimalControl/Analysis.html#lotka-volterra-equations",
    "href": "LotkaVolterraOptimalControl/Analysis.html#lotka-volterra-equations",
    "title": "Applying Optimal Control to Fishing",
    "section": "Lotka-Volterra Equations",
    "text": "Lotka-Volterra Equations\nLotka-Volterra equations are a pair of first-order nonlinear differential equations, used to describe the dynamics of two biologicals species’ interaction, one as a predator and the other as prey:\n\\[\n\\begin{align}\n\\frac{dx}{dt} &= \\alpha x - \\beta xy \\\\\n\\frac{dy}{dt} &= -\\gamma y + \\delta xy\n\\end{align}\n\\]\nwhere:\n\n\\(x\\) is the population density of prey;\n\\(y\\) is the population density of predator;\n\\(t\\) is the time;\n\\(\\alpha\\) and \\(\\beta\\) describes prey’s growth rate and effect of the presence of predators on prey’s growth rate, respectively.\n\\(\\gamma\\) and \\(\\delta\\) describes predators’s death rate and effect of the presence of prey on predator’s growth rate, respectively.\n\nIn our example, we will let \\(\\alpha=\\beta=\\gamma=\\delta=1\\).\nLet’s simulate the system with following parameters:\n\\(t_0=0, t_f=12\\)\n\\(x(0)=0.5, y(0)=0.7\\)\n\n\n\n\n\n\nCode - Lotka-Volterra Simulation\n\n\n\n\n\nimport numpy as np\nfrom scipy.integrate import odeint\n\nt0 = 0\ntf = 12\nX0 = [0.5, 0.7]\n\ndef lotka_volterra(X, t):\n    x, y = X\n    dx = x - x * y\n    dy = - y + x * y\n    return [dx, dy]\n\nt = np.linspace(t0, tf, 100)\nsol = odeint(lotka_volterra, X0, t)\n\nimport matplotlib.pyplot as plt\nplt.style.use('classic')\nplt.plot(t, sol[:, 0], 'b', label='Prey')\nplt.plot(t, sol[:, 1], 'g', label='Predator')\nplt.legend(loc='best')\nplt.xlabel('t')\nplt.grid()\nplt.savefig(\"prey_predator_uncontrolled.png\")\nplt.show()\n\n\n\nResult:\n\n\n\nUncontrolled Lotka-Volterra Dynamics\n\n\nAs we can see from graph, prey and predator population density values oscillates between 0.45 and 1.88."
  },
  {
    "objectID": "LotkaVolterraOptimalControl/Analysis.html#lotka-volterra-fishing-problem",
    "href": "LotkaVolterraOptimalControl/Analysis.html#lotka-volterra-fishing-problem",
    "title": "Applying Optimal Control to Fishing",
    "section": "Lotka-Volterra Fishing Problem",
    "text": "Lotka-Volterra Fishing Problem\nWe can define mixed-integer optimal control problem in Mayer form as follows:\n\\[\n\\min x_2(t_f)\n\\]\nsubject to\n\\[\n\\begin{align}\n\\frac{dx_0}{dt} &= x_0 - x_0x_1 - c_0x_0 w \\\\\n\\frac{dx_1}{dt} &= -x_1 + x_0x_1 - c_1x_1 w \\\\\n\\frac{dx_2}{dt} &= (x_0-1)^2 + (x_1-1)^2\n\\end{align}\n\\]\nwhere \\(x(0)=(0.5,0.7,0)^T\\) and \\(w(t)\\in\\{0,1\\}\\). The third state \\(x_2\\) is used to transform the objective into the Mayer formulation. The decision, whether the fishing fleet is actually fishing at time \\(t\\) is denoted by \\(w(t)\\).\n\n\n\n\n\n\nCode - Optimal Control Example\n\n\n\n\n\nfrom gekko import GEKKO\n\nm = GEKKO(remote=False)\nm.solver_options = ['minlp_gap_tol 0.001',\\\n                    'minlp_max_iter_with_int_sol 100',\\\n                    'minlp_branch_method 1',\\\n                    'minlp_integer_tol 0.001',\\\n                    'minlp_integer_leaves 0',\\\n                    'minlp_maximum_iterations 200']\n\nm.time = np.linspace(t0, tf, 100)\nx0 = m.Var(value=0.5,lb=0)\nx1 = m.Var(value=0.7,lb=0)\nx2 = m.Var(value=0.0,lb=0)\nw = m.MV(value=0,lb=0,ub=1,integer=True)\nw.STATUS = 1\nlast = m.Param(np.zeros(100))\nlast.value[-1] = 1\nm.Minimize(last*x2)\n\nm.Equations([x0.dt() == x0 - x0*x1 - 0.4*x0*w,\\\n             x1.dt() == - x1 + x0*x1 - 0.2*x1*w,\\\n             x2 == m.integral((x0-1)**2 + (x1-1)**2)])\n\nm.options.IMODE = 6\nm.options.NODES = 3\nm.options.SOLVER = 1\nm.options.MV_TYPE = 0\nm.solve()\n\nplt.figure(figsize=(6,4))\nplt.step(m.time,w.value,'r-',label='Control Variable (0/1)')\nplt.plot(m.time,x0.value,'b',label=r'Prey')\nplt.plot(m.time,x1.value,'g',label=r'Predator')\nplt.xlabel('t'); plt.ylabel('Biomass / Control Variable')\nplt.legend(loc='best'); plt.grid(); plt.tight_layout()\nplt.savefig(\"prey_predator_controlled.png\")\nplt.show()\n\n\n\nResult:\n\n\n\nControlled Lotka-Volterra Dynamics\n\n\nAs we can see from graph, with control variable, oscillation dampens and population density values converges to 1."
  },
  {
    "objectID": "LotkaVolterraOptimalControl/Analysis.html#references",
    "href": "LotkaVolterraOptimalControl/Analysis.html#references",
    "title": "Applying Optimal Control to Fishing",
    "section": "References",
    "text": "References\n\\(^1\\) Solve a Lotka-Volterra based ODE Optimal Control Problem \\(^2\\) Lotka Volterra fishing problem \\(^3\\) Lotka Volterra Fishing Optimization \\(^4\\) Introduction to Optimal Control"
  },
  {
    "objectID": "EM/Analysis.html",
    "href": "EM/Analysis.html",
    "title": "Under the hood: Expectation Maximization (EM)",
    "section": "",
    "text": "Introduction\n\n\n\nSource: https://jonathan-hui.medium.com/machine-learning-expectation-maximization-algorithm-em-2e954cb76959\n\n\nIf data contains missing values or latent (unobserved) variables, we cannot use the MLE for estimating parameters since the likelihood will be based on both observed and unobserved data. Expectation-maximization (EM) algorithm was developed by Dempster, Laird and Rubin for to find a maximum likelihood estimate of parameters in presence of missing or unobserved data \\(^2\\).\nAssume that the complete dataset consists of \\(\\mathcal{Z}=(\\mathcal{X},\\mathcal{Y})\\) but that only \\(\\mathcal{X}\\) is observed. Denote the (complete-data) log-likelihood as \\(l(\\theta;\\mathcal{X},\\mathcal{Y})\\) where \\(\\theta\\) is the unknown parameter vector which we want to estimate. Then, algorithm iteratively applies these two steps:\nExpectation step (E-step): Calculate the expected value of complete-data log-likelihood function \\(l(\\theta;\\mathcal{X},\\mathcal{Y})\\) given the observed data and the current parameter estimate \\(\\theta_{\\text{old}}\\):\n\\[\n\\begin{align*}\nQ(\\theta;\\theta_\\text{old}) &:= \\mathbb{E}[l(\\theta;\\mathcal{X},\\mathcal{Y})|\\mathcal{X},\\theta_\\text{old}]\\\\\n&= \\int l(\\theta;\\mathcal{X},y)p(y|\\mathcal{X},\\theta_{\\text{old}})dy\n\\end{align*}\n\\]\nwhere \\(p(\\cdot|\\mathcal{X},\\theta_{\\text{old}})\\) is the conditional density of \\(\\mathcal{Y}\\) given observed data \\(\\mathcal{X}\\), and assuming \\(\\theta=\\theta_\\text{old}\\).\nMaximization step (M-step): Maximize the expectation over \\(\\theta\\):\n\\[\n\\theta_\\text{new}:=\\max_\\theta Q(\\theta;\\theta_{\\text{old}})\n\\]\nand set \\(\\theta_\\text{old}=\\theta_\\text{new}\\). Repeat these two steps until the sequence of \\(\\theta_\\text{new}\\)’s converge \\(^1\\).\nOne can ask “How can we choose the initial values?”: for finite mixture distributions, we can estimate initial values for each distribution by K-means.\n\n\nExample - Finite Mixture Gaussians\nLet’s generate the data:\n\nset.seed(1234)\n\nn1 &lt;- 100\nmu1 &lt;- 5\nsigma1 &lt;- 2\n\nn2 &lt;- 50\nmu2 &lt;- 7\nsigma2 &lt;- 1.5\n\nd1 &lt;- rnorm(n1, mu1, sigma1)\nd2 &lt;- rnorm(n2, mu2, sigma2)\n\nd &lt;- c(d1, d2) # combine data\n\nLet’s look our generated data:\n\nhist(d)\n\n\n\n\nWe need to estimate initial values for EM algorithm. I’ll use K-means estimates for initial values:\n\nclusters &lt;- kmeans(d,2)$cluster\nmu1i &lt;- mean(d[clusters==1])\nmu2i &lt;- mean(d[clusters==2])\nsigma1i &lt;- sd(d[clusters==1])\nsigma2i &lt;- sd(d[clusters==2])\npi1i &lt;- sum(clusters==1)/length(clusters)\npi2i &lt;- sum(clusters==2)/length(clusters)\n\nApply algorithm:\n\n# Source: https://rpubs.com/H_Zhu/246450\nQ &lt;- 0\nQ[2] &lt;- sum(log(pi1i)+log(dnorm(d, mu1i, sigma1i))) + sum(log(pi2i)+log(dnorm(d, mu2i, sigma2i)))\n\nk &lt;- 2\n\nwhile (abs(Q[k]-Q[k-1])&gt;=1e-6) {\n  # E step\n  comp1 &lt;- pi1i * dnorm(d, mu1i, sigma1i)\n  comp2 &lt;- pi2i * dnorm(d, mu2i, sigma2i)\n  comp.sum &lt;- comp1 + comp2\n  \n  p1 &lt;- comp1/comp.sum\n  p2 &lt;- comp2/comp.sum\n  \n  # M step\n  pi1i &lt;- sum(p1) / length(d)\n  pi2i &lt;- sum(p2) / length(d)\n  \n  mu1i &lt;- sum(p1 * d) / sum(p1)\n  mu2i &lt;- sum(p2 * d) / sum(p2)\n  \n  sigma1 &lt;- sqrt(sum(p1 * (d-mu1i)^2) / sum(p1))\n  sigma2 &lt;- sqrt(sum(p2 * (d-mu2i)^2) / sum(p2))\n  \n  p1 &lt;- pi1i\n  p2 &lt;- pi2i\n  \n  k &lt;- k + 1\n  Q[k] &lt;- sum(log(comp.sum))\n}\n\nLet’s plot the resulting distributions over data:\n\nhist(d, prob=T, breaks=32, xlim=c(range(d)[1], range(d)[2]), main='')\nx1 &lt;- seq(from=range(d)[1], to=range(d)[2], length.out=1000)\ny1 &lt;- pi1i * dnorm(x1, mean=mu1i, sd=sigma1i) # first dist.\ny2 &lt;- pi2i * dnorm(x1, mean=mu2i, sd=sigma2i) # second dist.\nlines(x1, y1, col=\"red\", lwd=2)\nlines(x1, y2, col=\"blue\", lwd=2)\nlegend('topright', col=c(\"red\", 'blue'), lwd=2, legend=c(\"EM - 1st distribution\", \"EM - 2st distribution\"))\n\n\n\n\nFull source code: https://github.com/mrtkp9993/MyDsProjects/tree/main/EM\n\n\nReferences\n\\(^1\\) http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf\n\\(^2\\) https://web.mit.edu/6.435/www/Dempster77.pdf\n\\(^3\\) https://rpubs.com/H_Zhu/246450\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nCitationBibTeX citation:@online{koptur2022,\n  author = {Koptur, Murat},\n  title = {Under the Hood: {Expectation} {Maximization} {(EM)}},\n  date = {2022-10-10},\n  url = {https://www.muratkoptur.com/MyDsProjects/EM/Analysis.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKoptur, Murat. 2022. “Under the Hood: Expectation Maximization\n(EM).” October 10, 2022. https://www.muratkoptur.com/MyDsProjects/EM/Analysis.html."
  },
  {
    "objectID": "ConceptDrift/Analysis.html",
    "href": "ConceptDrift/Analysis.html",
    "title": "Why ML models fail in production: Model Drift",
    "section": "",
    "text": "Model drift is a huge problem for machine learning models in production. Model drift reveals itself as a significant increase in error rates for models. To reduce risk, it is essential to track model performance and detect concept drift.\nAssume that, given a set of features \\(X\\) and a target variable \\(y\\), we are trying to predict the target variable. Then, model drift can occur as following:\n\nIf \\(P(y|X)\\) conditional distribution changes over time, this is called concept drift;\nif \\(P(y)\\) distribution changes over time, this is called label drift;\nif \\(P(X)\\) distribution changes over time, this is called data drift.\n\nWe’ve said that “changes over time”, this change can occur at different shapes:\n\nSudden drift: Change occurs in a short period of time.\nGradual drift: Change occurs gradually.\nIncremental drift: Change occurs incrementally.\nReoccurring drifts: Some time after a change occurs, the old distribution comes again."
  },
  {
    "objectID": "ConceptDrift/Analysis.html#kolmogorov-smirnov-ks-test-chi-squared-test",
    "href": "ConceptDrift/Analysis.html#kolmogorov-smirnov-ks-test-chi-squared-test",
    "title": "Why ML models fail in production: Model Drift",
    "section": "Kolmogorov-Smirnov (KS) Test & Chi-squared Test",
    "text": "Kolmogorov-Smirnov (KS) Test & Chi-squared Test\nThese sets used for compare two statistical distributions. We can apply these tests to compare distributions of training data and post-training data."
  },
  {
    "objectID": "ConceptDrift/Analysis.html#population-stability-index-psi",
    "href": "ConceptDrift/Analysis.html#population-stability-index-psi",
    "title": "Why ML models fail in production: Model Drift",
    "section": "Population Stability Index (PSI)",
    "text": "Population Stability Index (PSI)\nPSI is a measure for determining how much a population shifted over time."
  },
  {
    "objectID": "ConceptDrift/Analysis.html#drift-detection-method-early-drift-detection-method",
    "href": "ConceptDrift/Analysis.html#drift-detection-method-early-drift-detection-method",
    "title": "Why ML models fail in production: Model Drift",
    "section": "Drift Detection Method / Early Drift Detection Method",
    "text": "Drift Detection Method / Early Drift Detection Method\n\nDrift Detection Method (DDM) uses a binomial distribution to describe the behavior of a random variable that gives the number of classification errors. If the distribution of the samples is stationary, probability of misclassification will decrease as sample size increases. If the error rate of the learning algorithm increases significantly, it suggests changes in the distribution of classes, and thus providing the signal to update the model \\(^5\\).\n\n\nEarly Drift Detection Method (EDDM) is a modification of DDM and improves the detection in presence of gradual concept drift \\(^5\\)."
  },
  {
    "objectID": "ConceptDrift/Analysis.html#page-hinkley-method",
    "href": "ConceptDrift/Analysis.html#page-hinkley-method",
    "title": "Why ML models fail in production: Model Drift",
    "section": "Page-Hinkley method",
    "text": "Page-Hinkley method\n\nThis change detection method works by computing the observed values and their mean up to the current moment \\(^6\\)."
  },
  {
    "objectID": "ConceptDrift/Analysis.html#adwin",
    "href": "ConceptDrift/Analysis.html#adwin",
    "title": "Why ML models fail in production: Model Drift",
    "section": "ADWIN",
    "text": "ADWIN\n\nADWIN (ADaptive WINdowing) is a popular drift detection method with mathematical guarantees. ADWIN efficiently keeps a variable-length window of recent items; such that it holds that there has no been change in the data distribution. This window is further divided into two sub-windows \\((W_0, W_1)\\) used to determine if a change has happened. ADWIN compares the average of \\(W_0\\) and \\(W_1\\) to confirm that they correspond to the same distribution. Concept drift is detected if the distribution equality no longer holds. Upon detecting a drift, \\(W_0\\) is replaced by \\(W_1\\) and a new \\(W_0\\) is initialized. ADWIN uses a significance value \\(\\delta\\in(0,1)\\) to determine if the two sub-windows correspond to the same distribution \\(^8\\)."
  },
  {
    "objectID": "ConceptDrift/Analysis.html#solutions-to-model-drift",
    "href": "ConceptDrift/Analysis.html#solutions-to-model-drift",
    "title": "Why ML models fail in production: Model Drift",
    "section": "Solutions to model drift",
    "text": "Solutions to model drift\n\nDiscard the old data and retrain the model blindly without detecting any concept drift periodically.\nWeight the all data inversly propotional to the age of data, then train the model.\nUse online (incremental) learning. As the new data arrives, retrain the existing model.\n\nFull source code: https://github.com/mrtkp9993/MyDsProjects/tree/main/ConceptDrift"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Murat Koptur",
    "section": "",
    "text": "I’m a mathematician and data scientist from Turkey. I have experience in developing and building end-to-end data science solutions. My interests are data science, numerical analysis, dynamical systems and chaos, and mathematical/statistical modeling.\nWhen not playing with data, I’m enjoying spending time playing music and working on digital art paintings."
  },
  {
    "objectID": "AnomalyOcsvm/Analysis.html",
    "href": "AnomalyOcsvm/Analysis.html",
    "title": "Unsupervised Anomaly Detection with One-Class SVM",
    "section": "",
    "text": "Introduction\nAnomaly detection (outlier detection, novelty detection) is the identification of rare observations that differ substantially from the vast majority of the data \\(^4\\).\nI would like to point out an important distinction \\(^3\\):\n\nOutlier detection: The training data contains outliers. Estimators try to fit the regions where the training data is the most concentrated.\nNovelty detection: The training data does not contain outliers. Estimators try to detect whether a new observation is an outlier.\n\nIn short, SVMs separates two classes using a hyperplane with the largest possible margin. On other side, One-Class SVMs try to identify smallest hypersphere which contains most of the data points\\(^4\\).\n\n\n\nSource: \\(^5\\)\n\n\n\n\nExample\nDataset was downloaded from ODDS \\(^{1,2}\\). The original dataset contains labels but we’ll not use them.\n\ndata = arff.loadarff('seismic-bumps.arff')\ndf = pd.DataFrame(data[0])\n\nfor col in df:\n    if isinstance(df[col][0], bytes):\n        df[col] = df[col].str.decode(\"utf8\")\n\nMarkdown(tabulate(\n  df.head(), \n  headers=df.columns\n))\n\n\n\n\n\nseismic\nseismoacoustic\nshift\ngenergy\ngpuls\ngdenergy\ngdpuls\nghazard\nnbumps\nnbumps2\nnbumps3\nnbumps4\nnbumps5\nnbumps6\nnbumps7\nnbumps89\nenergy\nmaxenergy\nclass\n\n\n\n\n0\na\na\nN\n15180\n48\n-72\n-72\na\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\na\na\nN\n14720\n33\n-70\n-79\na\n1\n0\n1\n0\n0\n0\n0\n0\n2000\n2000\n0\n\n\n2\na\na\nN\n8050\n30\n-81\n-78\na\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\na\na\nN\n28820\n171\n-23\n40\na\n1\n0\n1\n0\n0\n0\n0\n0\n3000\n3000\n0\n\n\n4\na\na\nN\n12640\n57\n-63\n-52\na\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\nLet’s drop categorical columns and class column:\n\ndf = df.loc[:, ~df.columns.isin(['seismic', 'seismoacoustic', 'shift', 'ghazard', 'class'])]\n\nMarkdown(tabulate(\n  df.head(), \n  headers=df.columns\n))\n\n\n\n\n\ngenergy\ngpuls\ngdenergy\ngdpuls\nnbumps\nnbumps2\nnbumps3\nnbumps4\nnbumps5\nnbumps6\nnbumps7\nnbumps89\nenergy\nmaxenergy\n\n\n\n\n0\n15180\n48\n-72\n-72\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n14720\n33\n-70\n-79\n1\n0\n1\n0\n0\n0\n0\n0\n2000\n2000\n\n\n2\n8050\n30\n-81\n-78\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n28820\n171\n-23\n40\n1\n0\n1\n0\n0\n0\n0\n0\n3000\n3000\n\n\n4\n12640\n57\n-63\n-52\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\nSplit data to train and test sets:\n\nX_train, X_test = train_test_split(df, test_size = 0.2)\n\nSVM tries to maximize distance between the hyperplane and the support vectors. If some features have very big values, they will dominate the other features. So it is important to rescale data while using distance based methods:\n\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nMarkdown(tabulate(\n  X_train_scaled[:5], \n  headers=df.columns,\n))\n\n\nScaled Train set \n\n\ngenergy\ngpuls\ngdenergy\ngdpuls\nnbumps\nnbumps2\nnbumps3\nnbumps4\nnbumps5\nnbumps6\nnbumps7\nnbumps89\nenergy\nmaxenergy\n\n\n\n\n0.0115891\n0.085282\n0.0775541\n0.0792291\n0.111111\n0\n0.142857\n0\n0\n0\n0\n0\n0.00660066\n0.00666667\n\n\n0.0122517\n0.146722\n0.0507084\n0.0770878\n0.222222\n0.25\n0\n0\n0\n0\n0\n0\n0.00363036\n0.00233333\n\n\n0.0112539\n0.10133\n0.231171\n0.143469\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0.00888829\n0.0790922\n0.0581655\n0.0760171\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0.0148177\n0.169188\n0.248322\n0.280514\n0.111111\n0\n0.142857\n0\n0\n0\n0\n0\n0.0231023\n0.0233333\n\n\n\n\n\n\nMarkdown(tabulate(\n  X_test_scaled[:5], \n  headers=df.columns,\n))\n\n\nScaled Test set \n\n\ngenergy\ngpuls\ngdenergy\ngdpuls\nnbumps\nnbumps2\nnbumps3\nnbumps4\nnbumps5\nnbumps6\nnbumps7\nnbumps89\nenergy\nmaxenergy\n\n\n\n\n0.0282483\n0.274874\n0.111111\n0.205567\n0.555556\n0.25\n0.285714\n0.333333\n0\n0\n0\n0\n0.09967\n0.0666667\n\n\n0.00104795\n0.00504356\n0.0790455\n0.0620985\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0.00152954\n0.0389729\n0.0260999\n0.0449679\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0.0598871\n0.0825309\n0.126771\n0.187366\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0.0218027\n0.182714\n0.189411\n0.22591\n0.111111\n0\n0.142857\n0\n0\n0\n0\n0\n0.00330033\n0.00333333\n\n\n\n\n\nApply T-SNE for 2-d visualization:\n\nt_sne = TSNE(n_components=2, \n             learning_rate = 'auto',\n             init='pca',\n             random_state=1234)\n             \nX_train_viz = t_sne.fit_transform(X_train_scaled)\nX_test_viz = t_sne.fit_transform(X_test_scaled)\n\n\npx.scatter(x=X_train_viz[:,0], y=X_train_viz[:,1], title=\"Train set\")\n\n\n                                                \n\n\n\npx.scatter(x=X_test_viz[:,0], y=X_test_viz[:,1], title=\"Test set\")\n\n\n                                                \n\n\nLet’s train and predict:\n\n# We assume that the proportion of outliers in the data set is 0.15\nclf = OCSVM(contamination=0.15)\nclf.fit(X_train_scaled)\n\nX_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\nX_train_scores = clf.decision_scores_  # raw outlier scores\n\nX_test_pred = clf.predict(X_test_scaled)  # outlier labels (0 or 1)\nX_test_scores = clf.decision_function(X_test_scaled)  # outlier scores\n\nReplace prediction classes (0 & 1) with strings:\n\nlabels = {0: 'inlier', 1: 'outlier'}\n\nX_train_pred = np.vectorize(labels.get)(X_train_pred)\nX_test_pred = np.vectorize(labels.get)(X_test_pred)\n\nVisualize with T-SNE:\n\npx.scatter(x=X_train_viz[:,0], y=X_train_viz[:,1], title=\"Train set\", color=X_train_pred)\n\n\n                                                \n\n\n\npx.scatter(x=X_test_viz[:,0], y=X_test_viz[:,1], title=\"Test set\",  color=X_test_pred)\n\n\n                                                \n\n\nFull source code: https://github.com/mrtkp9993/MyDsProjects/tree/main/AnomalyOcsvm\n\n\nReferences\n\\(^1\\) http://odds.cs.stonybrook.edu/seismic-dataset/\n\\(^2\\) Saket Sathe and Charu C. Aggarwal. LODES: Local Density meets Spectral Outlier Detection. SIAM Conference on Data Mining, 2016.\n\\(^3\\) https://scikit-learn.org/stable/modules/outlier_detection.html\n\\(^4\\) Contributors to Wikimedia projects. (2022, September 03). Anomaly detection - Wikipedia. Retrieved from https://en.wikipedia.org/w/index.php?title=Anomaly_detection&oldid=1108262189\n\\(^5\\) Yengi, Yeliz & Kavak, Adnan & Arslan, Huseyin. (2020). Physical Layer Detection of Malicious Relays in LTE-A Network Using Unsupervised Learning. IEEE Access. PP. 1-1. 10.1109/ACCESS.2020.3017045.\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nCitationBibTeX citation:@online{koptur2022,\n  author = {Koptur, Murat},\n  title = {Unsupervised {Anomaly} {Detection} with {One-Class} {SVM}},\n  date = {2022-09-10},\n  url = {https://www.muratkoptur.com/MyDsProjects/AnomalyOcsvm/Analysis.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKoptur, Murat. 2022. “Unsupervised Anomaly Detection with\nOne-Class SVM.” September 10, 2022. https://www.muratkoptur.com/MyDsProjects/AnomalyOcsvm/Analysis.html."
  },
  {
    "objectID": "EarthQuakeProbability/Analysis.html",
    "href": "EarthQuakeProbability/Analysis.html",
    "title": "Modelling the probability of earthquakes (M >= 5.0) in North Anatolian Fault Zone",
    "section": "",
    "text": "Let’s look and visualize the historical earthquake data.\n\n\n\n\n  \n\n\n\n\n\n\n\n```{r}\np &lt;- ggplot(data_diff_between_eq, aes(x=diff)) + geom_histogram(aes(y = ..density..)) + geom_density()\np\n```\n\n\n\n\n\n```{r}\np2 &lt;- ggplot(data_diff_between_eq, aes(x=diff)) + geom_boxplot()\np2\n```\n\n\n\n\n\n\n\n\n```{r}\np3 &lt;- ggplot(data_count_by_year, aes(x=year, y=count)) + geom_line()\np3\n```"
  },
  {
    "objectID": "EarthQuakeProbability/Analysis.html#time-span-between-earthquake-occurrences",
    "href": "EarthQuakeProbability/Analysis.html#time-span-between-earthquake-occurrences",
    "title": "Modelling the probability of earthquakes (M >= 5.0) in North Anatolian Fault Zone",
    "section": "",
    "text": "```{r}\np &lt;- ggplot(data_diff_between_eq, aes(x=diff)) + geom_histogram(aes(y = ..density..)) + geom_density()\np\n```\n\n\n\n\n\n```{r}\np2 &lt;- ggplot(data_diff_between_eq, aes(x=diff)) + geom_boxplot()\np2\n```"
  },
  {
    "objectID": "EarthQuakeProbability/Analysis.html#earthquake-count-by-year",
    "href": "EarthQuakeProbability/Analysis.html#earthquake-count-by-year",
    "title": "Modelling the probability of earthquakes (M >= 5.0) in North Anatolian Fault Zone",
    "section": "",
    "text": "```{r}\np3 &lt;- ggplot(data_count_by_year, aes(x=year, y=count)) + geom_line()\np3\n```"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Murat Koptur Data Science Blog & Projects",
    "section": "",
    "text": "Under the hood: Expectation Maximization (EM)\n\n\nHow to estimate parameters if our data contains missing values or variables?\n\n\n\nOct 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nUnder the hood: Maximum Likelihood Estimation (MLE)\n\n\nUnderstand the working mechanism of one of the most used statistical methods.\n\n\n\nOct 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nBuild Diversified Portfolio with Machine Learning: Clustering method for stock selection\n\n\nHow to create diversified portfolio using clustering.\n\n\n\nSep 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Classification with Random Forests\n\n\nHow to classify time series.\n\n\n\nSep 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty Quantification with Polynomial Chaos\n\n\nHow to perform uncertainty quantification using polynomial chaos expansion.\n\n\n\nSep 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nUnsupervised Anomaly Detection with One-Class SVM\n\n\nHow to identify anomalies with One-Class SVM.\n\n\n\nSep 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nNonlinear System Identification with NARMAX\n\n\nHow to identify a dynamical system from measurements.\n\n\n\nSep 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nWhy ML models fail in production: Model Drift\n\n\nHow to detect whether your data changed or not.\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t Try to Forecast Everything: Predictability of Time Series\n\n\nWe’ll look at a few handy tools that give more information about our time series.\n\n\n\nSep 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t impute all missing data: Missing Data Patterns\n\n\nHow to handle the data with missing values?\n\n\n\nAug 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nStock Return and Fundamental Clustering & Portfolio Selection\n\n\nBetter portfolio management & optimization with machine learning / clustering\n\n\n\nAug 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nModelling the probability of earthquakes (M &gt;= 5.0) in North Anatolian Fault Zone\n\n\nModelling distribution of days between two earthquakes occurred successively.\n\n\n\nAug 25, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "MissingData/Analysis.html",
    "href": "MissingData/Analysis.html",
    "title": "Don’t impute all missing data: Missing Data Patterns",
    "section": "",
    "text": "A missing data pattern is the structure of observed and missing values in a data set. This is not to be confused with a missing data mechanism, which describes possible relationships between data and an one’s propensity for missing values. Patterns describe where the gaps in the data are, whereas mechanisms explain why the values are missing.\n\nThe missing values in panel a have been isolated on a single variable in the univariate pattern. This pattern could appear, for example, in an experimental setting where outcome scores for a subset of participants are missing. Panel b depicts a monotone missing data pattern from a longitudinal study in which individuals who have missing data at one measurement event always have missing data at subsequent measurements. The general pattern in panel c is that missing values are scattered all through the entire data matrix. Panel d depicts a planned missing data pattern in which three variables are intentionally left blank for a large number of respondents. Panel e depicts a pattern in which a latent variable is absent across the entire sample.\nOne final configuration needs special consideration because it may introduce estimation issues for modern missing data-handling procedures. Because the data provide insufficient support for estimation, I refer to the configuration in panel f as an underidentified missing pattern. This pattern frequently occurs when two categorical variables have unbalanced group sizes and missing data, resulting in very low or even zero cell counts in a cross-tabulation table. Prior to conducting a missing data analysis, it is critical to screen for this configuration."
  },
  {
    "objectID": "MissingData/Analysis.html#distinguish-between-mnar-and-mar",
    "href": "MissingData/Analysis.html#distinguish-between-mnar-and-mar",
    "title": "Don’t impute all missing data: Missing Data Patterns",
    "section": "Distinguish between MNAR and MAR",
    "text": "Distinguish between MNAR and MAR\nThere is no statistical test for this, but you can:\n\nUse domain knowledge about variables;\nCollect more data for explaning missingness;\nDo literature search\n\nfor determining the missing data mechanisim."
  },
  {
    "objectID": "MissingData/Analysis.html#distinguish-between-mcar-and-mar",
    "href": "MissingData/Analysis.html#distinguish-between-mcar-and-mar",
    "title": "Don’t impute all missing data: Missing Data Patterns",
    "section": "Distinguish between MCAR and MAR",
    "text": "Distinguish between MCAR and MAR\nGenerally, two methods are preferred:\n\nLittle’s MCAR test: Maximum likelihood chi-square test for missing completely at random. \\(H_0\\) is that the data is MCAR.\nDummy variable approach for MCAR: One can create a dummy variable for whether a variable is missing (1 = missing, 0 = observed) and run t-tests (continuous) and chi-square (categorical) tests between this dummy and other variables to see if the missingness is related to the values of other variables."
  },
  {
    "objectID": "StockClustering/Analysis.html",
    "href": "StockClustering/Analysis.html",
    "title": "Stock Return and Fundamental Clustering & Portfolio Selection",
    "section": "",
    "text": "Introduction\n\n\n\nSource: https://www.pexels.com/tr-tr/fotograf/belgenin-ustundeki-buyutec-6801648/\n\n\nWe have following features for BIST30 stocks:\n\nMomentum 3-months, 6-months, 1-year\nVolatility 1-year, 2-year, 3-year\nPrice-To-Book Ratio\nMarket Capitalization\nReturn On Equity\nEarnings Growth\n\n\nlibrary(readr)\ndata &lt;- read_csv(\"data.csv\")\ndata &lt;- as.data.frame(data)\nrow.names(data) &lt;- data$stock\ndata\n\n\n\n  \n\n\n\nWe need to standardize the features:\n\n```{r}\nlibrary(scales)\n\ncols &lt;- 5:ncol(data)\n\ndata_scaled &lt;- lapply(data[, cols], function(x) if(is.numeric(x)) rescale(x, to=c(0,1)) else x)\ndata_scaled &lt;- as.data.frame(data_scaled)\ndata_scaled &lt;- cbind(data[,1:4], data_scaled)\nrow.names(data_scaled) &lt;- data_scaled$stock\ndata_scaled$stock &lt;- NULL\ndata_scaled\n```\n\n\n\n  \n\n\n\nEarnings Growth column has NA values, replace NA’s with zero:\n\ndata_scaled$earningsGrowth[is.na(data_scaled$earningsGrowth)] &lt;- 0\n\nWe will use K-means method. Let’s calculate silhouette score for optimal cluster count:\n\nlibrary(factoextra)\nlibrary(cluster)\n\nfviz_nbclust(data_scaled, kmeans, method = \"silhouette\", k.max = 10)\n\n\n\n\nWe have two different clusters, let’s fit model and see results:\n\nmodel &lt;- kmeans(data_scaled, 2)\nfviz_cluster(object = model,\n             data = data_scaled,\n             ellipse.type = \"norm\",\n             geom = \"text\",\n             palette = \"jco\",\n             main = \"\",\n             ggtheme = theme_minimal())\n\n\n\n\nStatistics:\n\nmodel\n\nK-means clustering with 2 clusters of sizes 24, 6\n\nCluster means:\n   return1y  return2y  return3y momentum_3m momentum_6m momentum_1y\n1 0.5021594 0.6856488 0.9261702   0.5014242   0.4282937   0.3035298\n2 0.6812538 1.2212164 1.8785916   0.5867881   0.5896588   0.4708884\n  volatility_1y volatility_2y volatility_3y priceToBook marketCap\n1     0.1032558     0.0909333    0.07866424  0.03512632 0.3220344\n2     0.4404008     0.4355551    0.39217887  0.34549470 0.6317486\n  returnOnEquity earningsGrowth\n1      0.3900359      0.1203581\n2      0.5562222      0.2889656\n\nClustering vector:\nTUPRS.IS VAKBN.IS KRDMD.IS DOHOL.IS AKBNK.IS TKFEN.IS FROTO.IS GARAN.IS \n       2        1        1        1        1        1        2        1 \nKOZAA.IS HALKB.IS TCELL.IS KOZAL.IS VESTL.IS YKBNK.IS TTKOM.IS BIMAS.IS \n       1        1        1        1        1        1        1        1 \nPETKM.IS ARCLK.IS EREGL.IS SAHOL.IS EKGYO.IS THYAO.IS PGSUS.IS ISCTR.IS \n       1        1        1        1        1        2        1        1 \nKCHOL.IS ASELS.IS TAVHL.IS GUBRF.IS  SISE.IS  SASA.IS \n       1        1        1        2        2        2 \n\nWithin cluster sum of squares by cluster:\n[1] 14.45341 10.20335\n (between_SS / total_SS =  26.7 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\nsil &lt;- silhouette(model$cluster, dist(data_scaled))\nfviz_silhouette(sil, palette = \"jco\", ggtheme = theme_classic())\n\n  cluster size ave.sil.width\n1       1   24          0.45\n2       2    6         -0.02\n\n\n\n\n\nThere are negative silhouette scores, it indicates that it may be in the wrong cluster.\nLet’s look the mean return statistics of each cluster:\n\nlibrary(dplyr)\n\npredicted_clusters &lt;- data.frame(predicted_cluster=model$cluster, row.names=names(model$cluster))\ndata_clustered &lt;- merge(data, predicted_clusters, by=0)\n\ndata_clustered %&gt;%\n  group_by(predicted_cluster) %&gt;%\n  summarise(\n    mean_1_year_return = mean(return1y),\n    std_1_year_return = sd(return1y),\n    mean_2_year_return = mean(return2y),\n    std_2_year_return = sd(return2y),\n    mean_3_year_return = mean(return3y),\n    std_3_year_return = sd(return3y),\n    )\n\n\n\n  \n\n\n\nFull source code: https://github.com/mrtkp9993/MyDsProjects/tree/main/StockClustering\n\n\nReferences\n\\(^1\\) https://www.investopedia.com/terms/c/cluster_analysis.asp\n\\(^2\\) https://www.investopedia.com/terms/f/factor-investing.asp\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nCitationBibTeX citation:@online{koptur2022,\n  author = {Koptur, Murat},\n  title = {Stock {Return} and {Fundamental} {Clustering} \\& {Portfolio}\n    {Selection}},\n  date = {2022-08-26},\n  url = {https://www.muratkoptur.com/MyDsProjects/StockClustering/Analysis.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKoptur, Murat. 2022. “Stock Return and Fundamental Clustering\n& Portfolio Selection.” August 26, 2022. https://www.muratkoptur.com/MyDsProjects/StockClustering/Analysis.html."
  },
  {
    "objectID": "SysIdent/Analysis.html",
    "href": "SysIdent/Analysis.html",
    "title": "Nonlinear System Identification with NARMAX",
    "section": "",
    "text": "System identification is a way to determine a system’s mathematical description by analyzing the system’s observed inputs and outputs. Definitely, the dynamics of the mathematical model that produced this signal from the measured input are hidden inside the output signal; how can this information be extracted? System identification provides a solution to this problem. Even under ideal conditions, this is difficult because the model of the system will be unknown, such as whether it is linear or nonlinear, how many terms are in the model, what type of terms should be in the model, whether the system has a time delay, what type of nonlinearity describes this system, and so on. Yet, if system is to be useful, these problems must be resolved. The benefits of system identification are numerous: it is applicable to all systems, it is frequently fast, and it can be used to track changes in the system\\(^1\\).\nWhat do we mean with “system”? One can think of “system” as any set of mathematical operations that takes one or more inputs and produces one or more outputs. Anything that can be related to input and output data is a system example: electrical systems, mechanical systems, biological systems, financial systems, chemical systems…\\(^3\\)\n\n\n\nSource: \\(^2\\)\n\n\nSystems can be considered of two types, depending on whether they satisfy the principle of superposition:\n\nLinear systems are satisfies the superposition principle: if system is linear, then if input \\(X_1\\) generates response \\(Y_1\\) and input \\(X_2\\) generates response \\(Y_2\\), then input \\(X_1 + X_2\\) generates response \\(Y_1 + Y_2\\).\nOn the other hand, nonlinear systems does not satisfy the superposition principle. This description is very vague, but there are so many types of nonlinear systems, it is almost impossible to write down a description that covers all the classes \\(^1\\).\n\nMost of real-world dynamical systems are nonlinear. In this article, we will not talk about linearization of nonlinear systems, as we will focus on the NARMAX method.\nFollowing steps are needed for NARMAX modelling \\(^3\\):\n\nCollecting data;\nChoice of mathematical representation;\nDetecting model structure;\nParameter estimation;\nModel validation;\nAnalysis of model.\n\nNARMAX method was introduced in 1981 by Stephen A. Billings, NARMAX models are able to represent the most different and complex nonlinear systems. NARMAX models can be represented as:\n\\[\ny_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_u}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k\n\\]\nwhere \\(n_y\\in \\mathbb{N}\\), \\(n_u \\in \\mathbb{N}\\) and \\(n_e \\in \\mathbb{N}\\) are the maximum lags for the system output and system input and system noise , respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(d\\) is the time delay; \\(e_k \\in \\mathbb{R}^{n_e}\\) is system uncertainty and noise at discrete time \\(k\\), (source: \\(^3\\)).\nTo approximate the unknown mapping \\(f[\\cdot]\\), we can use several nonlinear functions like \\(^1\\):\n\nPolynomial basis: Polynomial NARMAX model can be written as\n\\[\n\\begin{aligned}\ny(k)=\\theta_0 &+\\sum_{i_i=1}^n f_{i_1}\\left(x_{i_1}(k)\\right)+\\sum_{i_1=1}^n \\sum_{i_2=i_1}^n f_{i_1 i_2}\\left(x_{i_1}(k), x_{i_2}(k)\\right)+\\cdots \\\\\n&+\\sum_{i_1=1}^n \\cdots \\sum_{i_{\\ell}=i_{l-1}}^n f_{i_1 i_2 \\cdots i_l}\\left(x_{i_1}(k), x_{i_2}(k), \\ldots, x_{i_{\\ell}}(k)\\right)+e(k)\n\\end{aligned},\n\\]\n\\[\nf_{i_1 i_2 \\cdots i_m}\\left(x_{i_1}(k), x_{i_2}(k), \\ldots, x_{i_m}(k)\\right)=\\theta_{i_1 i_2 \\cdots i_m} \\prod_{k=1}^m x_{i_k}(k), 1 \\leq m \\leq \\ell,\n\\]\n\\[\nx_m(k)= \\begin{cases}y(k-m) & 1 \\leq m \\leq n_y \\\\ u\\left(k-\\left(m-n_y\\right)\\right) & n_y+1 \\leq m \\leq n_y+n_u \\\\ e\\left(k-\\left(m-n_y-n_u\\right)\\right) & n_y+n_u+1 \\leq m \\leq n_y+n_u+n_e\\end{cases}\n\\]\nwhere \\(l\\) is the degree of polynomial nonlinearity, \\(\\theta_{i_1,i_2,\\ldots,i_m}\\) are model parameters, and \\(n=n_y+n_u+n_e\\), (source: \\(^1\\)).\nGeneralized additive models:\nGeneralized additive models are defined as:\n\\[\ny(k)=a_0+a_1y(k-1)+\\cdots+a_{n_y}y(k-n_y)+b_1u(k-1)+\\cdots+b_{n_u}u(k-n_u)+e(k)\n\\]\nwhere \\(y(k)\\), \\(u(k)\\), \\(e(k)\\), \\(n_y\\), \\(n_u\\) and \\(n_e\\) are defined as before.\nNeural networks:\nRecurrent NARX can be defined as:\n\\[\ny(k)=F[\\mathbf{x}(k)]=w_0+\\sum_{i=1}^m w_i \\phi_i(\\mathbf{x}(k))+e(k)\n\\]\nwhere \\(\\phi_i(\\cdot)\\) is the nonlinear activation function.\nRadial basis functions:\nRBF networks can be defined as:\n\\[\ny(k)=F[\\mathbf{x}(k)]=\\sum_{i=1}^N w_i \\phi(\\|\\mathbf{x}(k)-\\mathbf{x}(i)\\|)\n\\]\nwhere \\(\\mathbf{x}(k)=[x_1(k),\\ldots,x_n(k)]^T\\) is the \\(k\\)th observation vector (\\(k=1,2,\\ldots,N\\)), \\(\\phi(||\\mathbf{x}(k)-\\mathbf{x}(i)||\\) are some arbitrary nonlinear functions known as radial basis functions or kernels, and \\(W_i\\) are the unknown weights.\nWavelet basis:\nWavelet NARMAX model can be defined as:\n\\[\ny(k)=F^{(P)}[\\mathbf{x}(k)]+F^{(W)}[\\mathbf{x}(k)]+F^{(Z)}[\\mathbf{z}(k)]+e(k)\n\\]\nwhere \\(F^{(P)}[\\mathbf{x}(k)]\\) is polynomial model that is used to model any slow or smooth varying trends, F^{(W)}[(k)] is a wavelet model that is used to model rapid changes or nonsmooth dynamics, and F^{(Z)}[(k)] is a linear or nonlinear moving average model that models the noise:\n\\[\nF^{(W)}[\\mathbf{x}(k)] = c_0+F_1[\\mathbf{x}(k)]+F_2[\\mathbf{x}(k)]+\\cdots+F_n[\\mathbf{x}(k)]\n\\]\n\\[\n\\begin{gathered}\nF_1(\\mathbf{x}(k))=\\sum_{i=1}^n f_i\\left(x_i(k)\\right) \\\\\nF_2(\\mathbf{x}(k))=\\sum_{i=1}^n \\sum_{j=i+1}^n f_{i j}\\left(x_i(k), x_j(k)\\right) \\\\\nF_m(\\mathbf{x}(k))=\\sum_{1 \\leq i_1&lt;i_2&lt;\\cdots&lt;i_m \\leq n} f_{i_1 i_2 \\cdots i_m}\\left(x_{i_1}(k), x_{i_2}(k), \\ldots, x_{i_m}(k)\\right), 2&lt;m&lt;n \\\\\nF_n(\\mathbf{x}(k))=f_{12 \\cdots n}\\left(x_1(k), x_2(k), \\ldots, x_n(k)\\right)\n\\end{gathered}\n\\]\nwhere \\(c_0\\) is constant and \\(F_i[\\cdot]\\) are individual wavelet sub-models."
  },
  {
    "objectID": "SysIdent/Analysis.html#lorenz-system",
    "href": "SysIdent/Analysis.html#lorenz-system",
    "title": "Nonlinear System Identification with NARMAX",
    "section": "Lorenz system",
    "text": "Lorenz system\nThe Lorenz system is a system of ordinary differential equations defined as:\n\\[\n\\begin{align*}\n\\frac{dx}{dt} &= \\sigma(y-x) \\\\\n\\frac{dy}{dt} &= x(\\rho-z)-y\\\\\n\\frac{dz}{dt} &= xy - \\beta z\n\\end{align*}\n\\] where \\(\\sigma, \\rho, \\beta\\) are model parameters.\nLet’s simulate the data with \\(\\rho=28, \\sigma=10, \\beta=8/3\\). Here is the time step \\(dt\\) is \\(0.01\\), final time \\(10\\), initial values \\((x_0,y_0,z_0)\\) is \\((0.1, 0.1, 0.1)\\):\n\nrho = 28\nsigma = 10\nbeta = 8/3\ndt = 0.01 \nT = 10\nn = int(T / dt) \nt = np.linspace(0, T, n)\n\nx = np.zeros(n)\ny = np.zeros(n)\nz = np.zeros(n)\n\nx[0] = 0.1\ny[0] = 0.1\nz[0] = 0.1\n\nfor i in range(n - 1):\n    x[i + 1] = x[i] + dt * (sigma * (y[i] - x[i]))\n    y[i + 1] = y[i] + dt * (x[i] * (rho - z[i]) - y[i])\n    z[i + 1] = z[i] + dt * (x[i] * y[i] - beta * z[i] )\n    \nfig, ax = plt.subplots(3, 1)\nax[0].plot(t, x, lw=2)\nax[1].plot(t, y, lw=2)\nax[2].plot(t, z, lw=2)\n\nax[0].set_title(\"x\", loc=\"right\")\nax[1].set_title(\"y\", loc=\"right\")\nax[2].set_title(\"z\", loc=\"right\")\n\nText(1.0, 1.0, 'z')\n\n\n\n\n\n\npx.line_3d(x=z,y=y,z=x, title=\"Lorenz system\") # xz axis inverted for sake of plot\n\n\n                                                \n\n\nDivide data to train and validation sets, last 50 observations will be validation set:\n\ntest_size = 50\n\nx_train, x_valid = temporal_train_test_split(pd.Series(x), test_size=test_size)\ny_train, y_valid = temporal_train_test_split(pd.Series(y), test_size=test_size)\nz_train, z_valid = temporal_train_test_split(pd.Series(z), test_size=test_size)\n\nplot_series(x_train, x_valid, labels=[\"x_train\", \"x_valid\"])\nplot_series(y_train, y_valid, labels=[\"y_train\", \"y_valid\"])\nplot_series(z_train, z_valid, labels=[\"z_train\", \"z_valid\"])\n\n(&lt;Figure size 1152x288 with 1 Axes&gt;, &lt;AxesSubplot:&gt;)\n\n\n\n\n\n\n\n\n\n\n\nComputations:\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function._basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import compute_residues_autocorrelation, compute_cross_correlation\n\n# Reshape data (needed for sysidentpy)\nx_train = x_train.values.reshape(-1, 1)\nx_valid = x_valid.values.reshape(-1, 1)\ny_train = y_train.values.reshape(-1, 1)\ny_valid = y_valid.values.reshape(-1, 1)\nz_train = z_train.values.reshape(-1, 1)\nz_valid = z_valid.values.reshape(-1, 1)\n\n# Our polynomial basis functions with max degrees 5\nbasis_function = Polynomial(degree=5) \n\n# Forward Regression with Orthogonal Least Squares (FOLRS) model structure identification\nmodelx = FROLS(\n    order_selection=True,\n    ylag=4,\n    elag=4,\n    info_criteria='aic',\n    estimator='recursive_least_squares',\n    basis_function=basis_function,\n    model_type='NAR' # we don't have exogenous variables\n)\n\n# Fit model to training data\nmodelx.fit(y=x_train)\n\n# Add needed lags to validation data\nx_valid = np.concatenate([x_train[-modelx.max_lag:], x_valid])\n\n# Predict the validation data\nxhat = modelx.predict(y=x_valid, forecast_horizon=test_size)\n\nCalculate MSE error score:\n\nfrom sktime.performance_metrics.forecasting import mean_squared_error\n\nmodelx_loss = mean_squared_error(x_valid, xhat)\nprint(modelx_loss)\n\n0.0033133566986800886\n\n\nRegression coefficients:\n\nr = pd.DataFrame(\n    results(\n        modelx.final_model, modelx.theta, modelx.err,\n        modelx.n_terms, err_precision=8, dtype='sci'\n        ),\n    columns=['Regressors', 'Parameters', 'ERR'])\nprint(r)\n\n           Regressors   Parameters             ERR\n0              y(k-1)   3.7758E+00  9.95972552E-01\n1              y(k-2)  -5.3235E+00  3.97965581E-03\n2              y(k-3)   3.3225E+00  4.63994821E-05\n3              y(k-4)  -7.7480E-01  1.31535686E-06\n4            y(k-1)^3  -1.3230E-05  1.32545331E-08\n5      y(k-3)^2y(k-2)   3.5535E-04  6.13642972E-08\n6            y(k-4)^5   9.6040E-09  6.44836942E-10\n7  y(k-4)y(k-3)y(k-1)   4.3154E-04  2.56203519E-10\n8      y(k-4)y(k-2)^2  -7.7419E-04  8.56524112E-10\n\n\nPlot prediction results:\n\nplot_results(y=x_valid[modelx.max_lag:], yhat=xhat[modelx.max_lag:])\n\n\n\n\nWe’ve a perfect fit.\nResidual autocorrelation and cross-correlations:\n\nee = compute_residues_autocorrelation(x_valid, xhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(x_valid, xhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n\n\n\n\n\n\n\n\\(y\\) and \\(z\\) components of the Lorenz model also can be estimated above. Now, let’s add measurement noise to our data and try to estimate it."
  },
  {
    "objectID": "SysIdent/Analysis.html#noisy-lorenz-system",
    "href": "SysIdent/Analysis.html#noisy-lorenz-system",
    "title": "Nonlinear System Identification with NARMAX",
    "section": "Noisy Lorenz System",
    "text": "Noisy Lorenz System\nNow we will assume that we observed the data with independent and identically distributed measurement noise (we will assume that measurement noise is distributed as uniform on interval [-1, 1]):\n\\[\n\\text{Observed Data} = \\text{Process Outcome} + \\text{Measurement Noise}\n\\]\nSimulate the noisy Lorenz system:\n\n# ...\n# add measurement noise to observations\nx = x + np.random.uniform(-1,1,n)\ny = y + np.random.uniform(-1,1,n)\nz = z + np.random.uniform(-1,1,n)\n\n\nfig, ax = plt.subplots(3, 1)\nax[0].plot(t, x, lw=2)\nax[1].plot(t, y, lw=2)\nax[2].plot(t, z, lw=2)\n\nax[0].set_title(\"x\", loc=\"right\")\nax[1].set_title(\"y\", loc=\"right\")\nax[2].set_title(\"z\", loc=\"right\")\n\npx.line_3d(x=z,y=y,z=x, title=\"Noisy Lorenz system\")\n\n\n                                                \n\n\n\n\n\nNow apply same method and look results:\nLoss:\n\nmodelx_loss = mean_squared_error(x_valid, xhat)\nprint(modelx_loss)\n\n3.1303289624476904\n\n\nRegression coefficients:\n\nr = pd.DataFrame(\n    results(\n        modelx.final_model, modelx.theta, modelx.err,\n        modelx.n_terms, err_precision=8, dtype='sci'\n        ),\n    columns=['Regressors', 'Parameters', 'ERR'])\nprint(r)\n\n                Regressors   Parameters             ERR\n0                   y(k-1)   4.5142E-01  9.86393489E-01\n1          y(k-11)^2y(k-7)  -9.1477E-04  2.95518502E-03\n2                   y(k-2)   3.0778E-01  1.52411152E-03\n3        y(k-11)^3y(k-9)^2   1.4444E-06  5.72445300E-04\n4                   y(k-3)   3.6355E-01  3.32725057E-04\n5                   y(k-9)  -1.5725E-01  3.16618507E-04\n6  y(k-10)^2y(k-3)y(k-2)^2  -6.5121E-07  1.64874661E-04\n7                   y(k-4)   1.0455E-01  1.71874614E-04\n\n\nPlot prediction results:\n\nplot_results(y=x_valid[modelx.max_lag:], yhat=xhat[modelx.max_lag:])\n\n\n\n\nIn the presence of noise, we added more lagged variables to the model and we tried to approximate the nature of the process.\nResidual autocorrelation and cross-correlations:\n\nee = compute_residues_autocorrelation(x_valid, xhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(x_valid, xhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n\n\n\n\n\n\n\nFull source code: https://github.com/mrtkp9993/MyDsProjects/tree/main/SysIdent"
  },
  {
    "objectID": "TimeSeriesPredictability/Analysis.html",
    "href": "TimeSeriesPredictability/Analysis.html",
    "title": "Don’t Try to Forecast Everything: Predictability of Time Series",
    "section": "",
    "text": "Most of time series analyses start with investigating series, autocorrelation and partial autocorrelation plots. Then one estimates different time series models (like ARIMA, GARCH, State-space models) and performs model checks.\nBut no one asks whether that series is predictable or not.\nWe’ll look at a few handy tools that give more information about our time series."
  },
  {
    "objectID": "TimeSeriesPredictability/Analysis.html#lyapunov-exponent",
    "href": "TimeSeriesPredictability/Analysis.html#lyapunov-exponent",
    "title": "Don’t Try to Forecast Everything: Predictability of Time Series",
    "section": "Lyapunov Exponent",
    "text": "Lyapunov Exponent\nLyapunov exponent of a dynamical system is a quantity that characterizes the rate of separation of infinitesimally close trajectories. Quantitatively, two trajectories in phase space with initial separation vector \\(\\delta Z_0\\) diverge at a rate given by\n\\[\n|\\delta Z(t)|\\approx e^{\\lambda t}|\\delta Z_0|\n\\]\nwhere \\(\\lambda\\) is the Lyapunov exponent. The rate of separation can be different for different orientations of initial separation vector. Thus, there is a spectrum of Lyapunov exponents. It is common to refer to the largest one as the maximal Lyapunov exponent (MLE), because it determines a notion of predictability for a dynamical system \\(^{7}\\).\n\n\nI’ll not go into detail on how to calculate the maximal Lyapunov exponent, we’ll look at practical implications.\nA positive MLE is usually taken as an indication that the system is chaotic \\(^{7}\\)."
  },
  {
    "objectID": "TimeSeriesPredictability/Analysis.html#hurst-exponent",
    "href": "TimeSeriesPredictability/Analysis.html#hurst-exponent",
    "title": "Don’t Try to Forecast Everything: Predictability of Time Series",
    "section": "Hurst Exponent",
    "text": "Hurst Exponent\nThe Hurst exponent is referred to as the “index of dependence” or “index of long-range dependence”. It quantifies the relative tendency of a time series either to regress strongly to the mean or to cluster in a direction:\n\nTrending (Persistent) series: If \\(0.5 &lt; H \\leq 1\\) , then series has long-term positive autocorrelation, so a high value in the series will probably be followed by another high value and the future will also tend to be high;\nRandom walk series: if \\(H = 0.5\\), then series is a completely uncorrelated series, so it can go either way (up or down);\nMean-reverting (Anti-persistent) series: if \\(0 \\leq H &lt; 0.5\\), then series has mean-reversion, so a high value in the series will probably be followed by a low value and vice versa \\(^{8}\\)."
  },
  {
    "objectID": "TimeSeriesPredictability/Analysis.html#detrended-fluctuation-analysis",
    "href": "TimeSeriesPredictability/Analysis.html#detrended-fluctuation-analysis",
    "title": "Don’t Try to Forecast Everything: Predictability of Time Series",
    "section": "Detrended Fluctuation Analysis",
    "text": "Detrended Fluctuation Analysis\nDFA is a method for determining the statistical self-affinity of a signal. It is the generalization of Hurst exponent, it means \\(^{8}\\):\n\nfor \\(0&lt;\\alpha&lt;0.5\\), then the series is anti-correlated;\nfor \\(\\alpha=0.5\\), then the series is uncorrelated and corresponds to white noise;\nfor \\(0.5&lt;\\alpha&lt;1\\), then the series is correlated;\nfor \\(\\alpha\\approx1\\), then the series corresponds to pink noise;\nfor \\(\\alpha&gt;1\\), then the series is nonstationary and unbounded;\nfor \\(\\alpha\\approx1.5\\), then the series corresponds to Brownian noise."
  },
  {
    "objectID": "TimeSeriesPredictability/Analysis.html#variance-ratio-test",
    "href": "TimeSeriesPredictability/Analysis.html#variance-ratio-test",
    "title": "Don’t Try to Forecast Everything: Predictability of Time Series",
    "section": "Variance Ratio Test",
    "text": "Variance Ratio Test\nThis test is often used to test the hypothesis that a given time series is a collection of i.i.d. observations or that it follows a martingale difference sequence.\nWe will use Chow and Denning’s multiple variance ratio test. There are two tests:\n\nCD1 - Test for i.i.d. series,\nCD2 - Test for uncorrelated series with possible heteroskedasticity.\n\nIf test statistics are bigger than critical values, the null hypothesis is rejected which means the series is not a random walk."
  },
  {
    "objectID": "TimeSeriesPredictability/Analysis.html#airpassengers-data",
    "href": "TimeSeriesPredictability/Analysis.html#airpassengers-data",
    "title": "Don’t Try to Forecast Everything: Predictability of Time Series",
    "section": "AirPassengers data",
    "text": "AirPassengers data\nResults:\n\nLyapunov exponent spectrum:\n\n\nCall:\nLyapunov exponent spectrum \n\nCoefficients:\n             Estimate Std. Error   z value      Pr(&gt;|z|)\nExponent 1 -0.8398548  0.2333552 -28.33887 5.739062e-177\nExponent 2 -1.5136329  0.1937088 -61.52719  0.000000e+00\n---\nProcedure: QR decomposition by bootstrap blocking method \nEmbedding dimension: 2, Time-delay: 1, No. hidden units: 10\nSample size: 129, Block length: 62, No. blocks: 1000\n\n\nThere are two statistically significant exponent estimates. The largest one is -0.84 which is negative, which means the series is not chaotic.\nHurst exponent is 0.8206234; it is bigger than 0.5, so series is trending.\nDFA is estimated as 1.2988566; it is nonstationary and unbounded.\nVariance ratio test:\n\n\n$Holding.Periods\n[1]  2  4  5  8 10 27\n\n$CD1\n[1] 24.48521\n\n$CD2\n[1] 21.22941\n\n$Critical.Values_10_5_1_percent\n[1] 2.378000 2.631038 3.142756\n\n\nBoth of test statistics are bigger than critical values, so the series is not a random walk."
  },
  {
    "objectID": "TimeSeriesPredictability/Analysis.html#lakehuron-data",
    "href": "TimeSeriesPredictability/Analysis.html#lakehuron-data",
    "title": "Don’t Try to Forecast Everything: Predictability of Time Series",
    "section": "LakeHuron data",
    "text": "LakeHuron data\nResults:\n\nLyapunov exponent spectrum:\n\n\nCall:\nLyapunov exponent spectrum \n\nCoefficients:\n             Estimate Std. Error    z value Pr(&gt;|z|)\nExponent 1 -0.2245224 0.03079226  -56.00722        0\nExponent 2 -0.6465142 0.01144893 -433.74968        0\nExponent 3 -0.6696687 0.01006248 -511.18811        0\nExponent 4 -1.6931702 0.02747627 -473.33519        0\n---\nProcedure: QR decomposition by bootstrap blocking method \nEmbedding dimension: 4, Time-delay: 1, No. hidden units: 2\nSample size: 94, Block length: 59, No. blocks: 1000\n\n\nThere are four statistically significant exponent estimates. The largest one is -0.22 which is negative, which means the series is not chaotic.\nHurst exponent is 0.7364948; it is bigger than 0.5, so series is trending.\nDFA is estimated as 1.1128455; it is nonstationary and unbounded.\nVariance ratio test:\n\n\n$Holding.Periods\n[1]  2  4  5  8 10  3\n\n$CD1\n[1] 11.45734\n\n$CD2\n[1] 9.407748\n\n$Critical.Values_10_5_1_percent\n[1] 2.378000 2.631038 3.142756\n\n\nBoth of test statistics are bigger than critical values, so the series is not a random walk."
  },
  {
    "objectID": "TimeSeriesPredictability/Analysis.html#simulated-time-series-data-from-the-logistic-map-with-chaos",
    "href": "TimeSeriesPredictability/Analysis.html#simulated-time-series-data-from-the-logistic-map-with-chaos",
    "title": "Don’t Try to Forecast Everything: Predictability of Time Series",
    "section": "Simulated time-series data from the Logistic map with chaos",
    "text": "Simulated time-series data from the Logistic map with chaos\nResults:\n\nLyapunov exponent spectrum:\n\n\nCall:\nLyapunov exponent spectrum \n\nCoefficients:\n            Estimate Std. Error   z value Pr(&gt;|z|)\nExponent 1 -1.291195  0.1580609 -63.27662        0\n---\nProcedure: QR decomposition by bootstrap blocking method \nEmbedding dimension: 1, Time-delay: 1, No. hidden units: 2\nSample size: 99, Block length: 60, No. blocks: 1000\n\n\nThere is one statistically significant exponent estimate, -1.29 which is negative, which means the series is not chaotic which is a questionable result.\nHurst exponent is 0.6255664; it is bigger than 0.5, so series is trending.\nDFA is estimated as 0.758476; it is correlated.\nVariance ratio test:\n\n\n$Holding.Periods\n[1]  2  4  5  8 10 10\n\n$CD1\n[1] 1.193817\n\n$CD2\n[1] 1.295116\n\n$Critical.Values_10_5_1_percent\n[1] 2.378000 2.631038 3.142756\n\n\nBoth of test statistics are smaller than critical values, so the series is a random walk.\n\nFull source code: https://github.com/mrtkp9993/MyDsProjects/tree/main/TimeSeriesPredictability"
  }
]