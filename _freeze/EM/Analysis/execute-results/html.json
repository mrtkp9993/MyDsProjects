{
  "hash": "02c439d32253f8bbd478244dc989bcc4",
  "result": {
    "markdown": "---\ntitle: \"Under the hood: Expectation Maximization (EM)\"\ndate: 10/10/2022\nauthor: Murat Koptur\nkeywords: data science, statistics, estimation, expectation maximization\ndescription: \"How to estimate parameters if our data contains missing values or variables?\"\nexecute:\n  freeze: auto\n  warning: false\n  error: false\nformat: \n   html:\n     df-print: paged\nlisting: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n# Introduction\n\n![Source: <https://jonathan-hui.medium.com/machine-learning-expectation-maximization-algorithm-em-2e954cb76959>](images/cover.webp)\n\nIf data contains missing values or latent (unobserved) variables, we cannot use\nthe MLE for estimating parameters since the likelihood will be based on both\nobserved and unobserved data. Expectation-maximization (EM) algorithm was developed\nby Dempster, Laird and Rubin for to find a maximum likelihood estimate of parameters \nin presence of missing or unobserved data $^2$.\n\nAssume that the complete dataset consists of $\\mathcal{Z}=(\\mathcal{X},\\mathcal{Y})$ but that only\n$\\mathcal{X}$ is observed. Denote the (complete-data) log-likelihood as $l(\\theta;\\mathcal{X},\\mathcal{Y})$ where $\\theta$ is the unknown parameter vector which we want to estimate. Then, algorithm iteratively applies these two steps:\n\n*Expectation step (E-step)*: Calculate the expected value of complete-data log-likelihood function $l(\\theta;\\mathcal{X},\\mathcal{Y})$ given the observed data and the current parameter estimate $\\theta_{\\text{old}}$:\n\n$$\n\\begin{align*}\nQ(\\theta;\\theta_\\text{old}) &:= \\mathbb{E}[l(\\theta;\\mathcal{X},\\mathcal{Y})|\\mathcal{X},\\theta_\\text{old}]\\\\\n &= \\int l(\\theta;\\mathcal{X},y)p(y|\\mathcal{X},\\theta_{\\text{old}})dy\n\\end{align*}\n$$\n\nwhere $p(\\cdot|\\mathcal{X},\\theta_{\\text{old}})$ is the conditional density of $\\mathcal{Y}$ given observed data $\\mathcal{X}$, and assuming $\\theta=\\theta_\\text{old}$.\n\n*Maximization step (M-step)*: Maximize the expectation over $\\theta$:\n\n$$\n\\theta_\\text{new}:=\\max_\\theta Q(\\theta;\\theta_{\\text{old}})\n$$\n\nand set $\\theta_\\text{old}=\\theta_\\text{new}$. Repeat these two steps until the sequence of $\\theta_\\text{new}$'s converge $^1$.\n\nOne can ask \"How can we choose the initial values?\": for finite mixture distributions,\nwe can estimate initial values for each distribution by K-means.\n\n# Example - Finite Mixture Gaussians\n\nLet's generate the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nn1 <- 100\nmu1 <- 5\nsigma1 <- 2\n\nn2 <- 50\nmu2 <- 7\nsigma2 <- 1.5\n\nd1 <- rnorm(n1, mu1, sigma1)\nd2 <- rnorm(n2, mu2, sigma2)\n\nd <- c(d1, d2) # combine data\n```\n:::\n\n\nLet's look our generated data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(d)\n```\n\n::: {.cell-output-display}\n![](Analysis_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nWe need to estimate initial values for EM algorithm. I'll use K-means estimates\nfor initial values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclusters <- kmeans(d,2)$cluster\nmu1i <- mean(d[clusters==1])\nmu2i <- mean(d[clusters==2])\nsigma1i <- sd(d[clusters==1])\nsigma2i <- sd(d[clusters==2])\npi1i <- sum(clusters==1)/length(clusters)\npi2i <- sum(clusters==2)/length(clusters)\n```\n:::\n\n\nApply algorithm:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Source: https://rpubs.com/H_Zhu/246450\nQ <- 0\nQ[2] <- sum(log(pi1i)+log(dnorm(d, mu1i, sigma1i))) + sum(log(pi2i)+log(dnorm(d, mu2i, sigma2i)))\n\nk <- 2\n\nwhile (abs(Q[k]-Q[k-1])>=1e-6) {\n  # E step\n  comp1 <- pi1i * dnorm(d, mu1i, sigma1i)\n  comp2 <- pi2i * dnorm(d, mu2i, sigma2i)\n  comp.sum <- comp1 + comp2\n  \n  p1 <- comp1/comp.sum\n  p2 <- comp2/comp.sum\n  \n  # M step\n  pi1i <- sum(p1) / length(d)\n  pi2i <- sum(p2) / length(d)\n  \n  mu1i <- sum(p1 * d) / sum(p1)\n  mu2i <- sum(p2 * d) / sum(p2)\n  \n  sigma1 <- sqrt(sum(p1 * (d-mu1i)^2) / sum(p1))\n  sigma2 <- sqrt(sum(p2 * (d-mu2i)^2) / sum(p2))\n  \n  p1 <- pi1i\n  p2 <- pi2i\n  \n  k <- k + 1\n  Q[k] <- sum(log(comp.sum))\n}\n```\n:::\n\n\nLet's plot the resulting distributions over data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(d, prob=T, breaks=32, xlim=c(range(d)[1], range(d)[2]), main='')\nx1 <- seq(from=range(d)[1], to=range(d)[2], length.out=1000)\ny1 <- pi1i * dnorm(x1, mean=mu1i, sd=sigma1i) # first dist.\ny2 <- pi2i * dnorm(x1, mean=mu2i, sd=sigma2i) # second dist.\nlines(x1, y1, col=\"red\", lwd=2)\nlines(x1, y2, col=\"blue\", lwd=2)\nlegend('topright', col=c(\"red\", 'blue'), lwd=2, legend=c(\"EM - 1st distribution\", \"EM - 2st distribution\"))\n```\n\n::: {.cell-output-display}\n![](Analysis_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nFull source code:\n<https://github.com/mrtkp9993/MyDsProjects/tree/main/EM>\n\n# References\n\n$^1$ <http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf>\n\n$^2$ <https://web.mit.edu/6.435/www/Dempster77.pdf>\n\n$^3$ <https://rpubs.com/H_Zhu/246450>\n",
    "supporting": [
      "Analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}