{
  "hash": "733d0ef8156c39bbdc33b293beb25e32",
  "result": {
    "markdown": "---\ntitle: \"Time Series Classification with Random Forests\"\ndate: 09/17/2022\nauthor: Murat Koptur\nkeywords: data science, statistics, time series, classification, machine learning\ndescription-meta: \"How to classify time series.\"\nexecute:\n  freeze: auto\n  warning: false\n  error: false\nformat: \n   html:\n     df-print: paged\nlisting: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n# Introduction\n\nFirst, let's look the methodology behind the idea.\n\nSuppose that $N$ training time series examples $\\{e_1,e_2,\\ldots,e_N\\}$\nand the corresponding class labels\n$\\{y_1,\\ldots,y_N\\},\\quad y_i\\in\\{1,2,\\ldots,C\\}$ where $C$ is the class\ncount, are given. The task is to predict the class labels for test\nexamples. Here, for simplicity, we assume the values of time series are\nmeasured at equally-spaced intervals and training and test time series\nexamples are of the same length $M$ $^1$.\n\n![Source:\n<https://www.sciencedirect.com/science/article/pii/B9780128119686000097>](images/paste-B898F7B3.png)\n\nTime series classification methods can be divided into two categories:\nInstance-based and feature-based. Instance-based methods like\nnearest-neighbor classifiers with Euclidean distance (NNEuclidean) or\ndynamic time warping (NNDTW) try to classify test examples based on its\nsimilarity to the training examples $^1$.\n\nFeature-based methods build models on temporal features like $^3$:\n\n-   Singular Value Decomposition (SVD),\n-   Discrete Fourier Transform (DFT),\n-   Coefficients of the decomposition into Chebysev Polynominals,\n-   Discrete Wavelet Transform (DWT),\n-   Piecewise Linear Approximation,\n-   ARMA coefficients,\n-   Symbolic representations like Symbolic Aggregate approXimation\n    (SAX).\n\n# Interval Features\n\nInterval features are computed from a time series interval, e.g, \"the\ninterval between time 5 and 15\". Let $K$ be the number of feature types\nand $f_K(\\cdot),\\quad k=1,2,\\ldots,K$ be the $k^{th}$ type. Here the\nauthors of the paper $^1$ considers three types: $f_1 = \\text{mean}$,\n$f_2 = \\text{standard deviation}$, and $f_3=\\text{slope}$.\n\nWe dont't go details of the algorithm here, we'll focus on examples.\n\n# Examples\n\n\nDatasets were downloaded from $^4$.\n\n## Atrial Fibrillation\n\n> This is a physionet dataset of two-channel ECG recordings has been\n> created from data used in the Computers in Cardiology Challenge 2004,\n> an open competition with the goal of developing automated methods for\n> predicting spontaneous termination of atrial fibrillation (AF). The\n> raw instances were 5 second segments of atrial fibrillation,\n> containing two ECG signals, each sampled at 128 samples per second.\n> The Multivate data organises these channels such that each is one\n> dimension. The class labels are: n, s and t. class n is described as a\n> non termination artiral fibrilation(that is, it did not terminate for\n> at least one hour after the original recording of the data). class s\n> is described as an atrial fibrilation that self terminates at least\n> one minuet after the recording process. class t is descirbed as\n> terminating immediatly, that is within one second of the recording\n> ending. PhysioNet Reference: Goldberger AL, Amaral LAN, Glass L,\n> Hausdorff JM, Ivanov PCh, Mark RG, Mietus JE, Moody GB, Peng CK,\n> Stanley HE. PhysioBank, PhysioToolkit, and PhysioNet: Components of a\n> New Research Resource for Complex Physiologic Signals. Circulation\n> 101(23):e215-e220 \\[Circulation Electronic Pages; (Link Here) 2000\n> (June 13). PMID: 10851218; doi: 10.1161/01.CIR.101.23.e215\n> Publication: Moody GB. Spontaneous Termination of Atrial Fibrillation:\n> A Challenge from PhysioNet and Computers in Cardiology 2004. Computers\n> in Cardiology 31:101-104 (2004) $^5$.\n\nLet's read dataset:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nX_tr, y_tr = load_from_tsfile_to_dataframe(\"data/AtrialFibrillation_TRAIN.ts\")\nX_ts, y_ts = load_from_tsfile_to_dataframe(\"data/AtrialFibrillation_TEST.ts\")\n```\n:::\n\n\nTake a look:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nMarkdown(tabulate(\n  X_tr.head(1), \n  headers=X_tr.columns\n))\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n    dim_0                        dim_1\n--  ---------------------------  ---------------------------\n 0  0     -0.34086               0      0.14820\n    1     -0.38038               1      0.13338\n    2     -0.34580               2      0.10868\n    3     -0.36556               3      0.09386\n    4     -0.34580               4      0.07410\n            ...                          ...\n    635   -0.04446               635   -0.03458\n    636   -0.04940               636   -0.05928\n    637   -0.02964               637   -0.06916\n    638   -0.01976               638   -0.06916\n    639    0.00000               639   -0.07410\n    Length: 640, dtype: float64  Length: 640, dtype: float64\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nMarkdown(tabulate(\n  y_tr, \n  headers=\"Labels\"\n))\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\nL\n---\nn\nn\nn\nn\nn\ns\ns\ns\ns\ns\nt\nt\nt\nt\nt\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfig, axes = plt.subplots(nrows=2, ncols=1)\n\nX_tr.iloc[0,0].plot(ax=axes[0])\nX_tr.iloc[0,1].plot(ax=axes[1])\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.88)\nfig.suptitle(\"non termination artiral fibrilation\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Analysis_files/figure-html/cell-6-output-1.png){width=662 height=474}\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfig, axes = plt.subplots(nrows=2, ncols=1)\n\nX_tr.iloc[6,0].plot(ax=axes[0])\nX_tr.iloc[6,1].plot(ax=axes[1])\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.88)\nfig.suptitle(\"an atrial fibrilation that self terminates\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Analysis_files/figure-html/cell-7-output-1.png){width=662 height=474}\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfig, axes = plt.subplots(nrows=2, ncols=1)\n\nX_tr.iloc[12,0].plot(ax=axes[0])\nX_tr.iloc[12,1].plot(ax=axes[1])\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.88)\nfig.suptitle(\"terminating immediatly\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Analysis_files/figure-html/cell-8-output-1.png){width=662 height=474}\n:::\n:::\n\n\nOur data is multivariate. We'll use `ColumnConcatenator` which\nconcatenates each dimension and converts multivariate time series to\nunivariate time series.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ncc = ColumnConcatenator()\nX_tr = cc.fit_transform(X_tr)\nX_ts = cc.fit_transform(X_ts)\n```\n:::\n\n\nLet's train classifier:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nclf = TimeSeriesForestClassifier(min_interval=200, n_estimators=10000, n_jobs=-1, random_state=1234)\nclf.fit(X_tr, y_tr)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\nTimeSeriesForestClassifier(min_interval=200, n_estimators=10000, n_jobs=-1,\n                           random_state=1234)\n```\n:::\n:::\n\n\nPredict test data and check accuracy:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ny_pr = clf.predict(X_ts)\nprint(classification_report(y_ts, y_pr))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           n       1.00      0.40      0.57         5\n           s       0.50      0.60      0.55         5\n           t       0.29      0.40      0.33         5\n\n    accuracy                           0.47        15\n   macro avg       0.60      0.47      0.48        15\nweighted avg       0.60      0.47      0.48        15\n\n```\n:::\n:::\n\n\nVisualize confusion matrix:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ncm = confusion_matrix(y_ts, y_pr, labels=['n', 's', 't'])\nax = sn.heatmap(pd.DataFrame(cm), annot=True, square=True, cbar=False, fmt='g')\nplt.xlabel(\"Predicted\") \nplt.ylabel(\"Actual\") \nax.invert_yaxis()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Analysis_files/figure-html/cell-12-output-1.png){width=422 height=422}\n:::\n:::\n\n\nFull source code:\n<https://github.com/mrtkp9993/MyDsProjects/tree/main/TimeSeriesClassification>\n\n# References\n\n$^1$ Deng, H., Runger, G., Tuv, E., & Vladimir, M. (2013). A Time Series\nForest for Classification and Feature Extraction. arXiv.\n<https://doi.org/10.48550/arXiv.1302.2277>\n\n$^2$ Johann Faouzi and Hicham Janati. pyts: A python package for time\nseries classification. Journal of Machine Learning Research, 21(46):1−6,\n2020.\n\n$^3$ Eruhimov, V., Martyanov, V., Tuv, E. (2007). Constructing High\nDimensional Feature Space for Time Series Classification. In: Kok, J.N.,\nKoronacki, J., Lopez de Mantaras, R., Matwin, S., Mladenič, D., Skowron,\nA. (eds) Knowledge Discovery in Databases: PKDD 2007. PKDD 2007. Lecture\nNotes in Computer Science(), vol 4702. Springer, Berlin, Heidelberg.\n<https://doi.org/10.1007/978-3-540-74976-9_41>\n\n$^4$ <https://www.timeseriesclassification.com/>\n\n$^5$\n<https://www.timeseriesclassification.com/description.php?Dataset=AtrialFibrillation>\n\n",
    "supporting": [
      "Analysis_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}