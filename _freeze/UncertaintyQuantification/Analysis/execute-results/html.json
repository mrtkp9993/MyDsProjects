{
  "hash": "b54eaef7df3cf98af5cca8432caad336",
  "result": {
    "markdown": "---\ntitle: \"Uncertainty Quantification with Polynomial Chaos\"\ndate: 09/13/2022\nauthor: Murat Koptur\nkeywords: data science, statistics, uncertainty quantification\ndescription-meta: \"How to perform uncertainty quantification using polynomial chaos expansion.\"\nexecute:\n  freeze: auto\n  warning: false\n  error: false\nformat: \n   html:\n     df-print: paged\nlisting: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n# Introduction\n\n![Source:\n$^4$](images/paste-B931C479.webp)\n\nAccording to $^2$, uncertainty\nquantification is defined as\n\n> The process of quantifying uncertainties associated with model\n> calculations of true, physical QOIs, with the goals of accounting for\n> all sources of uncertainty and quantifying the contributions of\n> specific sources to the overall uncertainty.\n\nand answers the question\n\n> How do the various sources of error and uncertainty feed into\n> uncertainty in the model-based prediction of the quantities of\n> interest?\n\n# Types of uncertainties\n\n* Aleatoric (statistical) uncertainty refers to the notion of randomness, \nthat is, the variability in the outcome of an experiment which is due to \ninherently random effects $^6$.\n\n* Epistemic uncertainty refers to uncertainty caused by a lack of knowledge, \ni.e., to the epistemic state of the agent $^6$.\n  \nIn real life applications, both kinds of uncertainties are present.  \n\n# Types of problems\n\nThere are two major types of problems in uncertainty quantification: one\nis the forward propagation of uncertainty (where the various sources of\nuncertainty are propagated through the model to predict the overall\nuncertainty in the system response) and the other is the inverse\nassessment of model uncertainty and parameter uncertainty (where the\nmodel parameters are calibrated simultaneously using test data) $^1$.\n\nPolynomial chaos is a method for quantifiying uncertainties on forward\nproblems. Its convergence is better than Monte Carlo methods $^3$.\n\n![Source: $^3$](images/paste-4B607140.webp)\n\n# Polynomial Chaos Expansion (PCE)\n\nConsider a problem in space $x$ and time $t$ where the aim is to quantify the\nuncertainty in response $Y$, computed bu a forward model $f$, which depends on\nuncertain input parameters $Q$:\n\n$$\nY = f(x,t,Q)\n$$\n\nWe want to quantify uncertainty in $Y$, but we know nothing about its density \ndistribution $p_Y$. The goal is to either build the density $p_Y$ or revelant \ndensity properties of $Y$ using the density $p_Q$ and the forward model $f$ $^5$.\n\nA general polynomial approximation can be defined as\n\n$$\n\\hat{f}(x,t,Q)=\\sum_{n\\in I_N}c_n(x,t)\\Phi_n(Q),\\quad I_N=\\{0,\\ldots N\\}\n$$\n\nwhere $\\{c_n\\}_{n\\in I_N}$ are coefficients and $\\{\\Phi_n\\}_{n\\in I_N}$ are polynomials.\nIf $\\hat{f}$ is a good approximation of $f$, it is possible to either infer statistical\nproperties of $\\hat{f}$ analytically or through numerical computations where $\\hat{f}$ \nis used as a surrogate for $f$ $^5$.\n\nA polynomial chaos expansion is defined as a polynomial approximation, where the \npolynomials $\\{\\Phi_n\\}_{n\\in I_N}$ are orthogonal on a custom weighted function\nspace $L_Q$:\n\n$$\n\\begin{align*}\n\\langle\t\\Phi_n,\\Phi_m \\rangle =& \\mathbb{E}\\Phi_n(Q)\\Phi_m(Q) \\\\ \n=& \\int\\ldots\\int \\Phi_n(q)\\Phi_m(q)p_Q(q)dq=0,\\quad n\\neq m\n\\end{align*}\n$$\n\n## Logistic Growth Model Example\n\nLogistic growth model is defined as\n\n$$\n\\frac{dX}{dt}=rX(1-\\frac{X}{K})\n$$\n\nwhere $r$ is the growth rate and $K$ is the population capacity (horizontal \nasymptote).\n\n\nLet's define the model and visualize it for some parameters and the initial condition\n$X_0=50$:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nt = numpy.linspace(0, 10, 100)\nx0 = 50\n\ndef logistic_model(x, t, r, K):\n    return r * x * (1 - x / K)\n  \nfig, axs = plt.subplots(2,2)\nfig.suptitle('Logistic model with different params')\n\naxs = axs.ravel()\n\nfor i, params in enumerate([(0.7, 60), (1.1, 60), (0.7, 300), (1.1, 300)]):\n  sol = odeint(logistic_model, x0, t, args=params)\n  axs[i].plot(t, sol[:, 0], 'b', label='x(t)')\n  axs[i].legend(loc='best')\n  axs[i].set_xlabel('t')\n  axs[i].set_ylabel('x')\n  axs[i].grid()\n  axs[i].plot()\n    \nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Analysis_files/figure-html/cell-3-output-1.png){width=597 height=470}\n:::\n:::\n\n\nNow let's assume that we have uncertainties over our parameters and assume that\n\n$$\n\\begin{align*}\nr &\\sim \\text{Log-Normal}(1, 0.1)\\\\\nK &\\sim \\text{Uniform}(100, 200)\n\\end{align*}\n$$\nLet's define our joint distribution:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nrdist= chaospy.LogNormal(1, 0.1)\nKdist = chaospy.Uniform(100, 200)\njoint = chaospy.J(rdist, Kdist)\n\ngrid = numpy.mgrid[joint.lower[0]:joint.upper[0]+1, joint.lower[1]:joint.upper[1]+1]\ncontour = plt.contourf(grid[0], grid[1], joint.pdf(grid), 50)\nplt.scatter(*joint.sample(50, seed=1234))\nplt.xlim(joint.lower[0], joint.upper[0])\nplt.ylim(joint.lower[1], joint.upper[1])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Analysis_files/figure-html/cell-4-output-1.png){width=575 height=409}\n:::\n:::\n\n\nGenerate expension, sample the joint distribution, evaluate model at these points and plot:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nexpansion = chaospy.generate_expansion(order=3, dist=joint)\n\n# and sample the joint distribution\nsamples = joint.sample(1000, rule=\"sobol\")\n\n# and evulate solver at these samples\nevaluations = numpy.array([odeint(logistic_model, x0, t, args=(sample[0], sample[1])) for sample in samples.T])\n\n# and plot\nplt.plot(t, evaluations[:,:,0].T, alpha=0.1)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Analysis_files/figure-html/cell-5-output-1.png){width=575 height=404}\n:::\n:::\n\n\nCreate polynomial approximation:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\napprox_solver = chaospy.fit_regression(expansion, samples, evaluations)\n```\n:::\n\n\nCalculate mean and deviance and plot:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nexpected = chaospy.E(approx_solver, joint)\ndeviation = chaospy.Std(approx_solver, joint)\n\nplt.fill_between(t, expected[:,0]-2*deviation[:,0], expected[:,0]+2*deviation[:,0], alpha=0.4)\nplt.plot(t, expected[:,0])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Analysis_files/figure-html/cell-7-output-1.png){width=575 height=404}\n:::\n:::\n\n\nFull source code: <https://github.com/mrtkp9993/MyDsProjects/tree/main/UncertaintyQuantification>\n\n# References\n\n$^1$ Contributors to Wikimedia projects. (2022, July 27). Uncertainty\nquantification - Wikipedia. Retrieved from\n<https://en.wikipedia.org/w/index.php?title=Uncertainty_quantification&oldid=1100714551>\n\n$^2$ Council, N. R., Engineering and Physical Sciences, D. O.,\nMathematical Sciences and Their Applications, B. O., & Mathematical\nFoundations of Verification, Validation, and Uncertainty Quantification,\nC. O. (2012). Assessing the Reliability of Complex Models. In\nMathematical and Statistical Foundations of Verification, Validation,\nand Uncertainty Quantification.\n\n$^3$\n<https://www.sintef.no/globalassets/project/evitameeting/2015/feinberg_lecture1.pdf>\n\n$^4$ <https://www.intechopen.com/media/chapter/54982/media/F1.png>\n\n$^5$ Feinberg, J., & Langtangen, H. P. (2015). Chaospy: An open source tool for designing methods of uncertainty quantification. Journal of Computational Science, 11, 46â€“57. <https://doi.org/10.1016/j.jocs.2015.08.008>\n\n$^6$ <https://www.gdsd.statistik.uni-muenchen.de/2021/gdsd_huellermeier.pdf>\n\n",
    "supporting": [
      "Analysis_files"
    ],
    "filters": [],
    "includes": {}
  }
}